{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 (default, Aug  7 2019, 17:28:10) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]\n"
     ]
    }
   ],
   "source": [
    "# create datacard for 2 tag\n",
    "\n",
    "import ROOT as rt\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "import uproot\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import awkward\n",
    "import numpy as np\n",
    "import time\n",
    "import numba\n",
    "from numba import jit\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append('/storage/af/user/christiw/gpu/christiw/llp/delayed_jet_analyzer/lib/')\n",
    "sys.path.append('/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_9_4_4/src/llp_analyzer/python/')\n",
    "from helper import make_datacard_2sig, make_datacard_2tag, weight_calc\n",
    "from histo_utilities import create_TH1D, create_TH2D, std_color_list, create_TGraph, make_ratio_plot\n",
    "from helper_functions import deltaR, deltaPhi\n",
    "\n",
    "import CMS_lumi, tdrstyle\n",
    "tdrstyle.setTDRStyle()\n",
    "CMS_lumi.writeExtraText = 0\n",
    "\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ntuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC_VBFH_SToEE_ms4p0_100 zombie\n",
      "MC_VBFH_SToEE_ms4p0_500 zombie\n",
      "MC_VBFH_SToGammaGamma_ms4p0_100 zombie\n",
      "MC_VBFH_SToGammaGamma_ms4p0_500 zombie\n",
      "MC_VBFH_SToPiPlusPiMinus_ms4p0_100 zombie\n",
      "MC_VBFH_SToPiPlusPiMinus_ms4p0_500 zombie\n",
      "MC_ZH_SToEE_ms4p0_100 zombie\n",
      "MC_ZH_SToEE_ms4p0_500 zombie\n",
      "MC_ZH_SToGammaGamma_ms4p0_100 zombie\n",
      "MC_ZH_SToGammaGamma_ms4p0_500 zombie\n",
      "MC_ZH_SToPiPlusPiMinus_ms4p0_100 zombie\n",
      "MC_ZH_SToPiPlusPiMinus_ms4p0_500 zombie\n",
      "MC_WH_SToEE_ms4p0_100 zombie\n",
      "MC_WH_SToEE_ms4p0_500 zombie\n",
      "MC_WH_SToGammaGamma_ms4p0_100 zombie\n",
      "MC_WH_SToGammaGamma_ms4p0_500 zombie\n",
      "MC_WH_SToPiPlusPiMinus_ms4p0_100 zombie\n",
      "MC_WH_SToPiPlusPiMinus_ms4p0_500 zombie\n",
      "MC_ttH_SToEE_ms4p0_100 zombie\n",
      "MC_ttH_SToEE_ms4p0_500 zombie\n",
      "MC_ttH_SToGammaGamma_ms4p0_100 zombie\n",
      "MC_ttH_SToGammaGamma_ms4p0_500 zombie\n",
      "MC_ttH_SToPiPlusPiMinus_ms4p0_100 zombie\n",
      "MC_ttH_SToPiPlusPiMinus_ms4p0_500 zombie\n",
      "MC_ggZH_SToEE_ms4p0_100 zombie\n",
      "MC_ggZH_SToEE_ms4p0_500 zombie\n",
      "MC_ggZH_SToGammaGamma_ms4p0_100 zombie\n",
      "MC_ggZH_SToGammaGamma_ms4p0_500 zombie\n",
      "MC_ggZH_SToPiPlusPiMinus_ms4p0_100 zombie\n",
      "MC_ggZH_SToPiPlusPiMinus_ms4p0_500 zombie\n"
     ]
    }
   ],
   "source": [
    "fpath =OrderedDict()\n",
    "tree = OrderedDict()\n",
    "mass = [15, 40, 55]\n",
    "\n",
    "\n",
    "old_ctau = {\n",
    "    'STodd_ms3p0':[100,500],\n",
    "    'SToEE_ms0p4':[10,50],\n",
    "    'SToGammaGamma_ms0p4':[10,50],\n",
    "    'SToKPlusKMinus_ms1p5':[38,187],\n",
    "    'SToK0K0_ms1p5':[38,187],\n",
    "    'SToPi0Pi0_ms0p4':[10,50],\n",
    "    'SToPi0Pi0_ms1p0':[25,125],\n",
    "    'SToPiPlusPiMinus_ms0p4':[10,50],\n",
    "    'SToPiPlusPiMinus_ms1p0':[25,125],\n",
    "    'STodd_ms7':[100, 1000, 10000, 100000],\n",
    "    'STodd_ms15':[100, 1000, 10000, 100000],\n",
    "    'STodd_ms40':[100, 1000, 10000, 100000],\n",
    "    'STodd_ms55':[100, 1000, 10000, 100000],\n",
    "    'SToTauTau_ms7':[100, 1000, 10000, 100000],\n",
    "    'SToTauTau_ms15':[100, 1000, 10000, 100000],\n",
    "    'SToTauTau_ms40':[100, 1000, 10000, 100000],\n",
    "    'SToTauTau_ms55':[100, 1000, 10000, 100000],\n",
    "    'SToBB_ms15':[100, 1000, 10000, 100000],\n",
    "    'SToBB_ms40':[100, 1000, 10000, 100000],\n",
    "    'SToBB_ms55':[100, 1000, 10000, 100000],\n",
    "    'SToEE_ms4p0':[100,500],\n",
    "    'SToGammaGamma_ms4p0':[100,500],\n",
    "    'SToPiPlusPiMinus_ms4p0':[100,500],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "prod = ['ggH', 'VBFH','ZH', 'WH', 'ttH','ggZH']\n",
    "# prod=['ggH']\n",
    "\n",
    "ntupler_version = 'V1p17/'\n",
    "mc_path = {}\n",
    "analyzer_version = 'v1/v163/'\n",
    "mc_central_path = '/storage/af/group/phys_exotica/delayedjets/displacedJetMuonAnalyzer/csc/'+ntupler_version+'/MC_all/'+analyzer_version+'/normalized/'\n",
    "analyzer_version = 'v2/v163/'\n",
    "mc_private_path = '/storage/af/group/phys_exotica/delayedjets/displacedJetMuonAnalyzer/csc/'+ntupler_version+'/MC_Fall18/'+analyzer_version+'/normalized/'\n",
    "\n",
    "\n",
    "for p in prod:\n",
    "#     if 'WH' in p:continue\n",
    "    for k in old_ctau.keys():\n",
    "        for ct in old_ctau[k]:\n",
    "\n",
    "            key = 'MC_'+p+'_'+k + '_'+str(ct)\n",
    "            if (p == 'ggH' or p == 'VBFH') and ('ms7' in k or 'ms15' in k or 'ms40' in k or 'ms55' in k) and not (p =='VBFH' and 'dd' in k):\n",
    "                mass = k[k.find('ms')+2:]\n",
    "                if 'dd' in k and p == 'ggH':fpath[key] = mc_central_path+p+'_HToSSTodddd_MH-125_MS-'+mass+'_ctau-'+str(ct)+'_137000pb_weighted.root'\n",
    "                elif 'BB' in k and p =='ggH':fpath[key] = mc_central_path+p+'_HToSSTobbbb_MH-125_MS-'+mass+'_ctau-'+str(ct)+'_137000pb_weighted.root'\n",
    "                elif 'BB' in k and p =='VBFH':fpath[key] = mc_central_path+p+'_HToSSTo4b_MH-125_MS-'+mass+'_ctau-'+str(ct)+'_137000pb_weighted.root'\n",
    "                else:fpath[key] = mc_central_path+p+'_HToSSTo4Tau_MH-125_MS-'+mass+'_ctau-'+str(ct)+'_137000pb_weighted.root'\n",
    "            else:\n",
    "                \n",
    "                if 'ggH' in p or 'ttH' in p: fpath[key] = mc_private_path+p+'_HToSS_'+k+'_pl'+str(ct)+'_137000pb_weighted.root'\n",
    "                elif p == 'VBFH' or p=='ZH' or p == 'WH': fpath[key] = mc_private_path+p+'ToSS_'+k+'_pl'+str(ct)+'_137000pb_weighted.root'\n",
    "                elif 'ggZH' in p:fpath[key] = mc_private_path+'ZHToSS_'+k+'_pl'+str(ct)+'_137000pb_weighted.root'\n",
    "            if not os.path.exists(fpath[key]):print(key, fpath[key])\n",
    "            \n",
    "\n",
    "            \n",
    "# mass = [15, 40, 55]\n",
    "# if not decay == 'bbbb': mass = [7, 15, 40, 55]\n",
    "\n",
    "# OLD_CTAU = np.array([100, 1000, 10000, 100000])#in mm\n",
    "\n",
    "# ntupler_version = 'V1p17/'\n",
    "\n",
    "data_path = '/storage/af/group/phys_exotica/delayedjets/displacedJetMuonAnalyzer/csc/V1p17/Data2018/v5/v163/normalized/'\n",
    "\n",
    "fpath['data'] = data_path + 'Run2_displacedJetMuonNtupler_V1p17_Data2016_Data2017_Data2018-HighMET_goodLumi.root'\n",
    "\n",
    "\n",
    "                     \n",
    "NEvents = {}\n",
    "# NEvents_genweight = {}\n",
    "for k,v in fpath.items():\n",
    "    root_dir = uproot.open(v) \n",
    "    if not root_dir: \n",
    "        print(k, \"zombie\")\n",
    "        continue\n",
    "    tree[k] = root_dir['MuonSystem']\n",
    "#     NEvents[k] = root_dir['NEvents'][1]\n",
    "    NEvents[k] = root_dir['NEvents']._fEntries\n",
    "#     print(k)\n",
    "#     if not 'data' in k: \n",
    "#         print(k, root_dir['NEvents']._fEntries)\n",
    "\n",
    "\n",
    "root_dir = uproot.open('/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_9_4_4/src/llp_analyzer/data/HiggsPtWeights/ZHToggZH_HiggsPtReweight.root') \n",
    "h_reweight = root_dir['higgsPthiggsEta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nCsc with different hit vetoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jetPt_cut = 50\n",
    "\n",
    "weight = {}\n",
    "nhits1 = {}\n",
    "nhits2 = {}\n",
    "sel_ev = {}\n",
    "cond = {}\n",
    "ggZH_weight = {}\n",
    "higgsEta = {}\n",
    "higgsPt = {}\n",
    "nDtWheels = {}\n",
    "nDtStations = {}\n",
    "nDTRechitsNoiseSec8 = {}\n",
    "nDTRechitsNoiseSec9 = {}\n",
    "nDTRechitsNoiseSec10 = {}\n",
    "dtRechitClusterDphi = {}\n",
    "dtRechitClusterMaxStation ={}\n",
    "dtRechitClusterMaxStationRatio = {}\n",
    "dtRechitClusterJetVetoPt = {}\n",
    "dtRechitClusterNSegmentStation1 = {}\n",
    "dtRechitClusterNSegmentStation2 = {}\n",
    "dtRechitClusterNSegmentStation3 = {}\n",
    "dtRechitClusterNSegmentStation4 = {}\n",
    "dtRechitClusterNSegStation1 = {}\n",
    "dtRechitClusterNSegStation2 = {}\n",
    "dtRechitClusterNSegStation3 = {}\n",
    "dtRechitClusterNSegStation4 = {}\n",
    "dtRechitClusterEta = {}\n",
    "dtRechitClusterMuonVetoPt = {}\n",
    "dtRechitClusterMuonVetoLooseId = {}\n",
    "cscRechitClusterMe11Ratio = {}\n",
    "cscRechitClusterMe12Ratio = {}\n",
    "cscRechitClusterMe11 = {}\n",
    "cscRechitClusterNStation = {}\n",
    "cscRechitClusterEta = {}\n",
    "cscRechitClusterMaxChamber = {}\n",
    "cosmicTwoLegCluster1Size = {}\n",
    "cosmicTwoLegCluster2Size = {}\n",
    "cscRechitClusterJetVetoPt = {}\n",
    "cscRechitClusterTimeTotal = {}\n",
    "cscRechitClusterMuonVetoPt = {}\n",
    "cscRechitClusterMuonVetoGlobal = {}\n",
    "cosmicTwoLegClusterChi2Reduced = {}\n",
    "cosmicTwoLegCluster2NStation = {}\n",
    "cosmicTwoLegCluster1NStation = {}\n",
    "cosmicTwoLegCluster2Index = {}\n",
    "cosmicTwoLegCluster1Index = {}\n",
    "nCosmic = {}\n",
    "deltaEta = {}\n",
    "deltaRCluster = {}\n",
    "jetPt = {}\n",
    "jetTightPassId = {}\n",
    "runNum = {}\n",
    "evtNum = {}\n",
    "lumiNum = {}\n",
    "cluster_index = ''\n",
    "addNoiseFlag = 1\n",
    "# 0: 2 CSC; 1: 2DT; 2: csc+dt\n",
    "category =0\n",
    "\n",
    "\n",
    "for k in list(tree.keys()):\n",
    "#     if  'data' in k:continue\n",
    "########### SELECTION: CLUSTERS ############\n",
    "    if 'data' in k: T = tree['data']\n",
    "    else: T = tree[k]\n",
    "\n",
    "    sel_csccluster = T.array('cscRechitCluster' + cluster_index + 'TimeSpreadWeightedAll')<20\n",
    "    sel_csccluster = np.logical_and(sel_csccluster, np.abs(T.array('cscRechitCluster' + cluster_index + 'MetEENoise_dPhi'))<1.2)\n",
    "    sel_csccluster = np.logical_and(sel_csccluster, T.array('cscRechitCluster' + cluster_index + 'JetVetoPt')<30)\n",
    "    sel_csccluster = np.logical_and(sel_csccluster, T.array('cscRechitCluster' + cluster_index + 'Me11Ratio')<1)\n",
    "    sel_csccluster = np.logical_and(sel_csccluster, np.logical_not(np.logical_and(T.array('cscRechitClusterMuonVetoPt') >= 30, T.array('cscRechitClusterMuonVetoGlobal'))))\n",
    "\n",
    "    if 'data' in k: \n",
    "        sel_csccluster = np.logical_and(sel_csccluster, np.logical_and(T.array('cscRechitCluster' + cluster_index + 'TimeWeighted')< 12.5, \\\n",
    "                                                                         T.array('cscRechitCluster' + cluster_index + 'TimeWeighted') > -5))\n",
    "    else: \n",
    "        sel_csccluster = np.logical_and(sel_csccluster, np.logical_and(T.array('cscRechitCluster' + cluster_index + 'TimeWeighted')+0.66 < 12.5, \\\n",
    "                                                                         T.array('cscRechitCluster' + cluster_index + 'TimeWeighted')+0.66 > -5))\n",
    "        \n",
    "    sel_dtcluster = np.abs(T.array('dtRechitClusterMetEENoise_dPhi')) < 1\n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_not(np.logical_and(T.array('dtRechitClusterMuonVetoPt') >= 10, T.array('dtRechitClusterMuonVetoLooseId'))))\n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.abs(T.array('dtRechitClusterJetVetoPt')) < 50)\n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_not(np.logical_and(T.array('dtRechitClusterMaxStation')==1, T.array('dtRechitClusterMaxStationRatio')>0.9)))\n",
    "    \n",
    "    \n",
    "    cut = 5\n",
    "    station = (T.array('dtRechitClusterNSegmentStation1')>cut).astype(int)+(T.array('dtRechitClusterNSegmentStation2')>cut).astype(int)\\\n",
    "+(T.array('dtRechitClusterNSegmentStation3')>cut).astype(int)+(T.array('dtRechitClusterNSegmentStation4')>cut).astype(int)\n",
    "\n",
    "    max_station = np.maximum(np.maximum(np.maximum(T.array('dtRechitClusterNSegmentStation1'), T.array('dtRechitClusterNSegmentStation2')), T.array('dtRechitClusterNSegmentStation3')), T.array('dtRechitClusterNSegmentStation4'))\n",
    "    min_station = np.minimum(np.minimum(np.minimum(T.array('dtRechitClusterNSegmentStation1'), T.array('dtRechitClusterNSegmentStation2')), T.array('dtRechitClusterNSegmentStation3')), T.array('dtRechitClusterNSegmentStation4'))\n",
    "\n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_or(station<4, min_station/max_station<0.4)) #remove if both clusters are 4 stations\n",
    "    if addNoiseFlag and not 'data' in k: \n",
    "        sel_dtcluster = np.logical_and(sel_dtcluster, (T.array('dtRechitClusterSize')+T.array('dtRechitClusterNoiseHit')) >= 50)\n",
    "    else: sel_dtcluster = np.logical_and(sel_dtcluster, T.array('dtRechitClusterSize') >= 50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###################### cosmic muon veto #############\n",
    "    sel_cosmic = np.logical_and(T.array('dtRechitClusterNOppositeSegStation1')>0, T.array('dtRechitClusterNOppositeSegStation2')>0)\n",
    "    sel_cosmic = np.logical_and(sel_cosmic, T.array('dtRechitClusterNOppositeSegStation3')>0)\n",
    "    sel_cosmic = np.logical_and(sel_cosmic, T.array('dtRechitClusterNOppositeSegStation4')>0)\n",
    "    sel_cosmic = np.logical_and(sel_cosmic, T.array('dtRechitClusterNOppositeSegStation1')+T.array('dtRechitClusterNOppositeSegStation2')+\\\n",
    "                               T.array('dtRechitClusterNOppositeSegStation3')+T.array('dtRechitClusterNOppositeSegStation4')>=6)\n",
    "    nstation = (T.array('dtRechitClusterNSegmentStation1')>1).astype(int)+(T.array('dtRechitClusterNSegmentStation2')>1).astype(int)\\\n",
    "    +(T.array('dtRechitClusterNSegmentStation3')>1).astype(int)+(T.array('dtRechitClusterNSegmentStation4')>1).astype(int)\n",
    "    \n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_not(np.logical_and(nstation>=3, sel_cosmic)))\n",
    "    \n",
    "########### SELECTION: JETS ############\n",
    "    \n",
    "    sel_jet = np.logical_and(T.array('jetPt') > 30, np.abs(T.array('jetEta')) < 2.4 )\n",
    "    sel_jet = np.logical_and(T.array('jetTightPassId'), sel_jet)\n",
    "\n",
    "\n",
    "            \n",
    "########### SELECTION: NOISE IN DT ############\n",
    "    \n",
    "    spike = np.logical_and( T.array('nDTRechitsSector')[:,0,0,7]>50,  T.array('nDTRechitsSector')[:,0,0,7]+T.array('nDTRechitsSector')[:,0,0,8]+T.array('nDTRechitsSector')[:,0,0,9]>120)\n",
    "    spike = np.logical_and(spike, T.array('nDTRechitsSector')[:,0,0,8]>25)\n",
    "    spike = np.logical_and(spike, T.array('nDTRechitsSector')[:,0,0,9]>10)\n",
    "\n",
    "    \n",
    "    \n",
    "########### SELECTION: EVENTS ############\n",
    "\n",
    "    sel_ev[k] = T.array('METNoMuTrigger')\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k] ,T.array('metEENoise') >= 200)\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k] , sel_jet.sum()>=1)\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k], (T.array('nDtRings')+T.array('nCscRings'))<10)\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k],T.array('Flag2_all'))\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k] , np.logical_not(spike))\n",
    "\n",
    "########### BRANCHES ############\n",
    "\n",
    "\n",
    "                     \n",
    "                            \n",
    "    if category == 0:\n",
    "        sel_ev[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum()== 2)\n",
    "        sel_ev[k]  = np.logical_and(sel_ev[k],sel_dtcluster.sum()== 0)\n",
    "        cond[k] = deltaR(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,0], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,0],\\\n",
    "                        T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,1], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,1])<2\n",
    "        \n",
    "       \n",
    "        nhits1[k] =  T.array('cscRechitCluster' + cluster_index + 'Size')[sel_csccluster][sel_ev[k]][cond[k]][:,0]\n",
    "        nhits2[k] =  T.array('cscRechitCluster' + cluster_index + 'Size')[sel_csccluster][sel_ev[k]][cond[k]][:,1]\n",
    "#         deltaRCluster[k] = deltaR(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,0], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,0],\\\n",
    "#                         T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,1], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,1])\n",
    "    elif category == 1:\n",
    "        sel_ev[k]  = np.logical_and(sel_ev[k],sel_dtcluster.sum()== 2)\n",
    "        sel_ev[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum()== 0)\n",
    "        cond[k] = np.abs(T.array('dtRechitCluster' + cluster_index + 'Eta')[sel_dtcluster][sel_ev[k]][:,0]-\\\n",
    "                                                    T.array('dtRechitCluster' + cluster_index + 'Eta')[sel_dtcluster][sel_ev[k]][:,1])>=0\n",
    "        if addNoiseFlag and not 'data' in k:\n",
    "            nhits1[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster][sel_ev[k]][:,0]\n",
    "            nhits2[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster][sel_ev[k]][:,1]\n",
    "        else:\n",
    "            nhits1[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][:,0]\n",
    "            nhits2[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][:,1]\n",
    "    elif category == 2:\n",
    "        sel_ev[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum() == 1)\n",
    "        sel_ev[k]  = np.logical_and(sel_ev[k],sel_dtcluster.sum() == 1)\n",
    "        \n",
    "#         cond[k] =  np.abs(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,0]-\\\n",
    "#                                                     T.array('dtRechitCluster' + cluster_index + 'Eta')[sel_dtcluster][sel_ev[k]][:,0])<2.0\n",
    "       \n",
    "        cond[k] = deltaR(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,0], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,0],\\\n",
    "                        T.array('dtRechitCluster' + cluster_index + 'Eta')[sel_dtcluster][sel_ev[k]][:,0], T.array('dtRechitCluster' + cluster_index + 'Phi')[sel_dtcluster][sel_ev[k]][:,0])<2.5\n",
    "        if addNoiseFlag and not 'data' in k:\n",
    "            nhits1[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster][sel_ev[k]][cond[k]][:,0]\n",
    "        else:\n",
    "            nhits1[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][cond[k]][:,0]\n",
    "        nhits2[k] =  T.array('cscRechitCluster' + cluster_index + 'Size')[sel_csccluster][sel_ev[k]][cond[k]][:,0]\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        assert(False)\n",
    "\n",
    "    \n",
    "    higgsPt[k] = T.array('gHiggsPt')[sel_ev[k]][cond[k]]\n",
    "    higgsEta[k] = T.array('gHiggsEta')[sel_ev[k]][cond[k]]\n",
    "    evtNum[k] = T.array('evtNum')[sel_ev[k]][cond[k]]\n",
    "    lumiNum[k] = T.array('lumiSec')[sel_ev[k]][cond[k]]\n",
    "    runNum[k] = T.array('runNum')[sel_ev[k]][cond[k]]\n",
    "    ggZH_weight[k]=h_reweight.values[np.argmax(h_reweight.edges[0]>higgsPt[k][:,None],axis=1)-1, np.argmax(h_reweight.edges[1]>np.abs(higgsEta[k])[:,None],axis=1)-1]\n",
    "    \n",
    "#     if 'ggH' in k: weight[k] = (T.array('weight')*T.array('pileupWeight')*T.array('metSF'))[sel_ev[k]]\n",
    "\n",
    "    jetTightPassId[k] = T.array('jetTightPassId')[sel_ev[k]][cond[k]]\n",
    "    jetPt[k] = T.array('jetPt')[sel_ev[k]][cond[k]]\n",
    "    if 'ggH' in k: weight[k] = (T.array('weight')*T.array('pileupWeight')*T.array('higgsPtWeight')*T.array('metSF'))[sel_ev[k]][cond[k]]\n",
    "    else:weight[k] = (T.array('weight')*T.array('pileupWeight')*T.array('metSF'))[sel_ev[k]][cond[k]]\n",
    "    if 'ggZH' in k: weight[k] *= ggZH_weight[k]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datacard_version  = 'v1' # ABCD for all categories\n",
    "datacard_version  = 'v2' # ABCD for CSC-DT and 3 bins for CSC-CSC and DT-DT\n",
    "datacard_version  = 'v3' # signal uncertainties and corrections applied\n",
    "datacard_version  = 'v4' # global muon veto for csc and loose id muon veto for dt\n",
    "datacard_version  = 'v5' # global muon veto + eta cut for csc and loose id muon veto for dt\n",
    "datacard_version  = 'v6' # global muon veto for csc and loose id muon veto + station veto +tight ID jet veto for dt\n",
    "\n",
    "datacard_version  = 'v7' # deltaEta cut 2.5 (DT-CSC) and 1.5 (CSC-CSC)\n",
    "datacard_version  = 'v8' # deltaEta cut 2 (DT-CSC) and 1.5 (CSC-CSC)\n",
    "\n",
    "datacard_version  = 'v9' # deltaR cut for CSC and DT\n",
    "# datacard_version  = 'v10' # deltaR cut for CSC and DT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# signal uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: 0\n",
      "bins: 3\n"
     ]
    }
   ],
   "source": [
    "cscCluster_mc = {}\n",
    "cscCluster_data = {}\n",
    "time_spread_mc = {}\n",
    "time_spread_data = {}\n",
    "dtCluster = {}\n",
    "higgsPtWeight = {}\n",
    "JES = {}\n",
    "pileup = {}\n",
    "# ctau_reweight = {}\n",
    "mc_stats = {}\n",
    "scaling = {}\n",
    "lumi = {}\n",
    "time_spread = {}\n",
    "time = {}\n",
    "theory = {}\n",
    "pdf = {}\n",
    "readout = {}\n",
    "ggH_higgsPt , VBFH_higgsPt , WH_higgsPt , ZH_higgsPt , ttH_higgsPt , ggZH_higgsPt =({} for i in range(6)) \n",
    "ggH_xsec , VBFH_xsec , WH_xsec , ZH_xsec , ttH_xsec , ggZH_xsec =({} for i in range(6)) \n",
    "ggH_pdf , VBFH_pdf , WH_pdf , ZH_pdf , ttH_pdf , ggZH_pdf =({} for i in range(6)) \n",
    "Nrechit_weight_file = '/storage/user/christiw/login-1/christiw/LLP/CMSSW_9_4_4/src/llp_analyzer/data/Uncertainties/Nrechit_weight.txt'\n",
    "\n",
    "cut_based_unc = [0.0921, 0.0852] # muon Eta reweighted\n",
    "clustering_unc = [0.1506, 0.1047]\n",
    "if category == 2 or datacard_version == 'v1': bins = 4\n",
    "else: bins = 3\n",
    "print('category: '+ str(category))\n",
    "print('bins: '+ str(bins))\n",
    "\n",
    "            \n",
    "for m in old_ctau.keys():\n",
    "    for ct in old_ctau[m]:\n",
    "        for p in prod:\n",
    "            if p == 'others':k = 'MC_ggH_'+str(m)+'_'+str(ct)   \n",
    "            else:k = 'MC_'+p+'_'+str(m)+'_'+str(ct)         \n",
    "            if p == 'others': key = 'MC_others_'+str(m)+'_'+str(ct)   \n",
    "            else: key = k\n",
    "            if p == 'others':k = 'MC_ggH_'+str(m)+'_'+str(ct)   \n",
    "            else:k = 'MC_'+p+'_'+str(m)+'_'+str(ct)         \n",
    "            if p == 'others': key = 'MC_others_'+str(m)+'_'+str(ct)   \n",
    "            else: key = k\n",
    "            \n",
    "            # CSC hits\n",
    "            if category == 0:\n",
    "                cscCluster_mc[key] = [0.02] * bins\n",
    "                cscCluster_data[key] = [0.012] * bins\n",
    "                time_spread_mc[key] = [0.037*2] * bins\n",
    "                time_spread_data[key] = [0.039*2] * bins\n",
    "                dtCluster[key] = [0.0] * bins\n",
    "                JES[key] = [0.0351,0.0573, 0.0470] \n",
    "                pileup[key] = [0.0102,0.0144,0.0183]\n",
    "\n",
    "            elif category == 1:\n",
    "                cscCluster_mc[key] = [0.0] * bins\n",
    "                cscCluster_data[key] = [0.0] * bins\n",
    "                time_spread_mc[key] = [0.0] * bins\n",
    "                time_spread_data[key] = [0.0] * bins\n",
    "                dtCluster[key] = [0.032] * bins\n",
    "                JES[key] = [0.027, 0.036,0.079]\n",
    "                pileup[key] = [0.013, 0.0152, 0.0783]\n",
    "            else: \n",
    "                cscCluster_mc[key] = [0.013] * bins\n",
    "                cscCluster_data[key] = [0.017] * bins\n",
    "                time_spread_mc[key] = [0.037] * bins\n",
    "                time_spread_data[key] = [0.039] * bins\n",
    "                dtCluster[key] = [0.011] * bins\n",
    "                JES[key] = [0.0332, 0.0306, 0.1211,0.039]\n",
    "                pileup[key] = [0.011,0.019, 0.018,0.025]\n",
    "\n",
    "\n",
    "            lumi[key] = [0.018]*bins\n",
    "            \n",
    "\n",
    "\n",
    "            ggH_higgsPt[key], VBFH_higgsPt[key], WH_higgsPt[key] , ZH_higgsPt[key] , ttH_higgsPt[key] , ggZH_higgsPt[key] = ([0.0]*bins*2,)*6\n",
    "            ggH_xsec[key], VBFH_xsec[key], WH_xsec[key] , ZH_xsec[key] , ttH_xsec[key] , ggZH_xsec[key] = ([0.0]*bins*2,)*6\n",
    "            ggH_pdf[key] , VBFH_pdf[key] , WH_pdf[key] , ZH_pdf[key] , ttH_pdf[key] , ggZH_pdf[key] = ([0.0]*bins,)*6\n",
    "            if 'ggH' in key: \n",
    "#                 if category == 2 or datacard_version == 'v1': ggH_higgsPt[key] = [0.205, 0.205,0.207,0.208, 0.133,0.133,0.134,0.134] #down*4/up*4\n",
    "#                 else:  ggH_higgsPt[key] = [0.205, 0.205,0.207, 0.133,0.133,0.134] #down*4/up*4\n",
    "                    \n",
    "                if category == 0:ggH_higgsPt[key] = [0.2066,0.2069, 0.2069,0.1339,0.134,0.1341] #down*3/up*3\n",
    "                elif category == 1:ggH_higgsPt[key] = [0.2074,0.2077, 0.2061,0.1342,0.1343,0.1336] #down*3/up*3\n",
    "                else:ggH_higgsPt[key] = [0.2071, 0.2066, 0.2084, 0.2076, 0.134,0.134,0.1345,0.1343] #down*4/up*4\n",
    "                ggH_xsec[key] = [0.067]*bins+[0.046]*bins #down/up\n",
    "                ggH_pdf[key] = [0.032]*bins\n",
    "\n",
    "            elif 'VBFH' in key: \n",
    "#                 if category == 2 or datacard_version == 'v1':VBFH_higgsPt[key] = [0.006, 0.006, 0.006, 0.006, 0.010, 0.010, 0.010, 0.010] #down/up\n",
    "#                 else: VBFH_higgsPt[key] = [0.006, 0.006, 0.006, 0.010, 0.010, 0.010] #down/up\n",
    "                if category == 0:VBFH_higgsPt[key] = [0.0364,0.0484,0.0811,0.0084,0.0241,0.0161] #down*3/up*3\n",
    "                elif category == 1:VBFH_higgsPt[key] = [0.0154, 0.0332,0.0823,0.0102,0.024,0.0485]\n",
    "                else:VBFH_higgsPt[key] = [0.0152,0.0306,0.1045,0.0208, 0.0096,0.0071, 0.0351,0.0109]\n",
    "                VBFH_xsec[key] = [0.003]*bins+[0.004]*bins #down/up\n",
    "                VBFH_pdf[key] = [0.021]*bins\n",
    "            elif 'ggZH' in key:\n",
    "                if category == 0:ggZH_higgsPt[key] = [0.114,0.0501,0.2873,0.0199,0.0134,0.1375]\n",
    "                elif category == 1:ggZH_higgsPt[key] = [0.0426,0.2433,0.2063,0.0229,0.0396,0.0075]\n",
    "                else:ggZH_higgsPt[key] = [0.0995, 0.0998, 0.8319,0.1495,0.0194,0.0144,0.2761,0.0753]\n",
    "                ggZH_xsec[key] = [0.251]*bins+[0.189]*bins #down/up\n",
    "                ggZH_pdf[key] = [0.024]*bins\n",
    "            elif 'ZH' in key: \n",
    "                if category == 0:ZH_higgsPt[key] = [0.0658, 0.055, 0.3026, 0.0135, 0.013, 0.1221]\n",
    "                elif category == 1:ZH_higgsPt[key] = [0.0674, 0.0771, 0.03348, 0.0265, 0.022, 0.0912]\n",
    "                else:ZH_higgsPt[key] = [0.1875, 0.0629, 0.2784, 0.731, 0.0049, 0.0107, 0.0934, 0.0251]\n",
    "                ZH_xsec[key] = [0.006]*bins+[0.005]*bins #down/up\n",
    "                ZH_pdf[key] = [0.019]*bins\n",
    "            elif 'WH' in key: \n",
    "                if category == 0:WH_higgsPt[key] = [0.099, 0.0525, 0.3833, 0.0162, 0.0132, 0.0854]\n",
    "                elif category == 1:WH_higgsPt[key] = [0.0241, 0.1452, 0.1078, 0.0099, 0.0457, 0.0478]\n",
    "                else:WH_higgsPt[key] = [0.0772, 0.1123, 0.5623, 0.204, 0.0102, 0.0313, 0.1362, 0.0541]\n",
    "                WH_xsec[key] = [0.007]*bins+[0.005]*bins #down/up\n",
    "                WH_pdf[key] = [0.019]*bins\n",
    "            elif 'ttH' in key: \n",
    "                if category == 0:ttH_higgsPt[key] = [0.0919, 0.2877, 0.3674, 0.0245, 0.0976, 0.1691]\n",
    "                elif category == 1:ttH_higgsPt[key] = [0.0144,0.0076, 0.4021, 0.0024, 0.048, 0.166]\n",
    "                else:ttH_higgsPt[key] = [0.0092, 0.2868, 0.2367, 0.0844, 0.0107, 0.0635, 0.1291, 0.0287]\n",
    "                ttH_xsec[key] = [0.092]*bins+[0.058]*bins #down/up\n",
    "                ttH_pdf[key] = [0.036]*bins\n",
    "            mc_stats[k] = []\n",
    "            if p == 'ggZH': scaling[key] = [0.2]*bins\n",
    "            else: scaling[key] = [0] * bins\n",
    "\n",
    "\n",
    "sig_eff = [lumi, cscCluster_mc, cscCluster_data, time_spread_mc, time_spread_data, dtCluster, JES,pileup, \\\n",
    "           ggH_higgsPt, VBFH_higgsPt, WH_higgsPt, ZH_higgsPt, ttH_higgsPt, ggZH_higgsPt,  \\\n",
    "           ggH_xsec, VBFH_xsec, WH_xsec, ZH_xsec, ttH_xsec, ggZH_xsec, \\\n",
    "           ggH_pdf,VBFH_pdf, WH_pdf, ZH_pdf, ttH_pdf, ggZH_pdf,\\\n",
    "           scaling, mc_stats]\n",
    "sig_eff_name = ['lumi','cscCluster_nhits_mc', 'cscCluster_nhits_data', 'csc_timeSpread_mc', 'csc_timeSpread_data', 'dtCluster_nhits', 'JES','pileup', \\\n",
    "                'ggH_higgsPt', 'VBFH_higgsPt', 'WH_higgsPt', 'ZH_higgsPt', 'ttH_higgsPt', 'ggZH_higgsPt',\\\n",
    "                'ggH_xsec', 'VBFH_xsec', 'WH_xsec', 'ZH_xsec', 'ttH_xsec', 'ggZH_xsec', \\\n",
    "                'ggH_pdf','VBFH_pdf', 'WH_pdf', 'ZH_pdf', 'ttH_pdf', 'ggZH_pdf',\\\n",
    "                'ggZH_reweight','category'+str(category)+'mc_stats']\n",
    "\n",
    "\n",
    "# csc: jet, muon, time, time spread, clustereff_unc, readout\n",
    "# theory uncertainty for now\n",
    "assert(len(sig_eff)==len(sig_eff_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make datacard (ABCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_10_2_13/src/HiggsAnalysis/MuonSystemLimit/combine/datacards_2tag/V1p17/v2/v163//v9/unblindABC/category0/\n",
      "category: 0\n",
      "background:  [0.04166667 1.         6.        ]\n",
      "[0.04166667 1.         6.        ] [0.04166667 1.         6.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:214: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:214: RuntimeWarning: overflow encountered in square\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:214: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import importlib\n",
    "importlib.reload(sys.modules['helper'])\n",
    "from helper import  make_datacard_2tag, make_datacard_2tag_3bins, weight_calc\n",
    "N_RECHIT_CUTS = np.arange(60,220, 10)\n",
    "\n",
    "ctaus = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','30','40', '50','60', '100', '125','150','200','300','500','600','700','800','900','1000', '2000','3000','4000', '5000', '6000','7000','8000','10000', '20000','30000','50000',\\\n",
    "         '100000', '200000', '300000', '500000', '1000000', '2000000', '3000000', '5000000', '6000000', '10000000'] #mm\n",
    "UNBLIND = 1\n",
    "# 0 use exponential fit, 1 unblind ABC, 2 unblind ABCD\n",
    "N_RECHIT_CSC = 100\n",
    "N_RECHIT_DT = 80\n",
    "\n",
    "\n",
    "#####\n",
    "# a = c*c1*c2\n",
    "# b = c1* c\n",
    "# c = c\n",
    "# d = c2*c\n",
    "# start adding signal\n",
    "#####\n",
    "outDataCardsDir = '/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_10_2_13/src/HiggsAnalysis/MuonSystemLimit/combine/datacards_2tag/'+\\\n",
    "ntupler_version+analyzer_version+'/'\n",
    "outDataCardsDir += datacard_version +'/'\n",
    "if UNBLIND == 0: outDataCardsDir += 'blind/'\n",
    "elif UNBLIND == 1: outDataCardsDir += 'unblindABC/'\n",
    "else:outDataCardsDir += 'unblind/'\n",
    "outDataCardsDir += 'category'+str(category)+'/'\n",
    "if not os.path.isdir(outDataCardsDir):os.makedirs(outDataCardsDir)\n",
    "print(outDataCardsDir)\n",
    "if category == 2 or datacard_version == 'v1': bins = 4\n",
    "else: bins = 3\n",
    "\n",
    "\n",
    "#############\n",
    "# BACKGROUND\n",
    "#############\n",
    "\n",
    "bkg_unc = [] #ABCD closure, sum of stats from two VR\n",
    "bkg_unc_name = []\n",
    "if UNBLIND == 0:\n",
    "    ##### v138 #####\n",
    "    if 'v138' in analyzer_version:\n",
    "        if category == 0:bkg_rate = np.array([0.062*0.073,0.073,1,0.062])*59 #2csc\n",
    "        elif category == 1:bkg_rate = np.array([0.02*0.01,0.02,1,0.01])*44 # dt\n",
    "        elif category == 2:bkg_rate = np.array([0.07*0.15,0.07,1,0.15])*66 #csc+dt\n",
    "    elif 'v141' in analyzer_version:\n",
    "        if datacard_version == 'v1':\n",
    "            if category == 0:bkg_rate = np.array([0.03*0.03,0.03,1 ,0.03])*18 #2csc\n",
    "            elif category == 1:bkg_rate = np.array([0.03*0.03,0.03,1, 0.03])*19 # dt\n",
    "            elif category == 2:bkg_rate = np.array([0.009*0.025,0.009,1,0.025])*35 #csc+dt\n",
    "        elif datacard_version == 'v2':\n",
    "            if category == 0:bkg_rate = np.array([0.03*0.03,0.03*2,1])*18 #2csc\n",
    "            elif category == 1:bkg_rate = np.array([0.03*0.03,0.03*2,1])*19 # dt\n",
    "            elif category == 2:bkg_rate = np.array([0.009*0.025,0.009,1,0.025])*35 #csc+dt\n",
    "    elif 'v146' in analyzer_version: #with dtcosmic muon veto\n",
    "        if datacard_version == 'v2':\n",
    "#             if category == 0:bkg_rate = np.array([0.02, 1.2, 18])\n",
    "#             elif category == 1:bkg_rate = np.array([0.02, 1.2, 17.8])\n",
    "#             elif category == 2:bkg_rate = np.array([0.01, 0.3, 34.2, 0.9])\n",
    "            if category == 0:bkg_rate = np.array([0.09, 2.5, 18])\n",
    "            elif category == 1:bkg_rate = np.array([0.02, 1.2, 17.9])\n",
    "            elif category == 2:bkg_rate = np.array([0.02, 0.7, 34.2, 0.9])\n",
    "elif UNBLIND == 1:\n",
    "    k = 'data'\n",
    "    if category <= 1:\n",
    "        if category == 0: N_RECHIT_CUT = N_RECHIT_CSC\n",
    "        else: N_RECHIT_CUT = N_RECHIT_DT\n",
    "#             cond_a = np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT)#bin A\n",
    "        b = np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT)#bin A\n",
    "        b = np.count_nonzero(np.logical_or(b, np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT)))\n",
    "        c = np.count_nonzero(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT))\n",
    "        bkg_rate = np.array([b**2/4.0/c, b, c])\n",
    "\n",
    "    else:\n",
    "        N_RECHIT_CUT_1 = N_RECHIT_DT\n",
    "        N_RECHIT_CUT_2 = N_RECHIT_CSC\n",
    "        cond_a = np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2)#bin A\n",
    "        b = np.count_nonzero(np.logical_and(nhits1[k]<N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2))\n",
    "        d = np.count_nonzero(np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]<N_RECHIT_CUT_2))\n",
    "        c = np.count_nonzero(np.logical_and(nhits1[k]<N_RECHIT_CUT_1, nhits2[k]<N_RECHIT_CUT_2))\n",
    "        bkg_rate = np.array([b*d/c, b, c, d])\n",
    "    print('category: '+str(category))\n",
    "    print('background: ', bkg_rate)\n",
    "\n",
    "else: assert(False)\n",
    "observation= bkg_rate #blinded observation\n",
    "print(observation, bkg_rate)\n",
    "#############\n",
    "# SIGNAL\n",
    "#############\n",
    "\n",
    "\n",
    "sig_norm = []\n",
    "\n",
    "\n",
    "###########################################################\n",
    "        \n",
    "br_path = '/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_9_4_4/src/llp_analyzer/data/branchingRatio_phi/'\n",
    "h_mass = {}\n",
    "for filename in os.listdir(br_path): # loop over mass points from 0.1 to 40 GeV\n",
    "    f = os.path.join(br_path, filename)\n",
    "    if os.path.isfile(f) and '.txt' in filename and filename[:3] == 'phi':\n",
    "        m = float(filename[4:-4].replace('dot', '.'))\n",
    "        if not round(m,1) == m and m > 1.0:continue\n",
    "#         if m >40 and m <55:continue\n",
    "        if m == 0:continue\n",
    "        if m <= 0.2:continue\n",
    "        br = []\n",
    "        decay = []\n",
    "        total_limits = np.array([])\n",
    "        with open(f) as fi: \n",
    "            lines = fi.readlines()\n",
    "            \n",
    "            #####\n",
    "            if len(prod) == 5: modelName = 'allProd_HToSS_'\n",
    "            elif len(prod) ==6:  modelName = 'allProd_withggZH_HToSS_'\n",
    "            else: modelName = ('_').join(prod)+'_HToSS_'\n",
    "            modelName = modelName +str(mass)+'_ctau'+str(ct)+'mm_nCsc'+str(N_RECHIT_CSC)+'_nDt'+str(N_RECHIT_DT)\n",
    "            signal_rate = {}\n",
    "            mc_stat_unc = {}\n",
    "            gmn = {}\n",
    "            sig_unc = {}\n",
    "            ####\n",
    "            for i_line,line in enumerate(lines): #loop over all decays\n",
    "                if not '9000006:addChannel' in line:continue\n",
    "                br_line = line[line.find('=')+2:].split(\" \")\n",
    "                assert(abs(float(br_line[3])) == abs(float(br_line[4])))\n",
    "                if abs(float(br_line[3])) == 13:continue\n",
    "                if abs(float(br_line[3])) == 3 or abs(float(br_line[3])) == 4 or abs(float(br_line[3])) == 21:\n",
    "                    decay.append(1)\n",
    "                else:decay.append(abs(float(br_line[3])))\n",
    "                br.append(float(br_line[1]))\n",
    "                if decay[-1] not in pid:print('PARTICLE NOT FOUND', br_line[3])\n",
    "\n",
    "                # find which mass to use\n",
    "                use_m_s = 0.0\n",
    "                list_of_m = mass[decays[pid.index(decay[-1])]]\n",
    "                if m>np.max(list_of_m): \n",
    "                    use_m_s = np.max(list_of_m)\n",
    "                    index = np.argmax(list_of_m)\n",
    "                else:\n",
    "                    for i, m_s in enumerate(list_of_m):\n",
    "                        if m <= m_s:\n",
    "                            use_m_s = m_s\n",
    "                            index = i\n",
    "                            break\n",
    "                # find sample corresponding to the mass and decay mode\n",
    "                sample_name = 'allProd_withggZH_HToSS_STo'+decays[pid.index(decay[-1])]+'_ms'+mass_string[decays[pid.index(decay[-1])]][index]\n",
    "                assert(sample_name in samples)\n",
    "\n",
    "\n",
    "#                 if len(total_limits) == 0:\n",
    "#                     x += array('d', [math.log10(float(m))]*len(ctaus_m))\n",
    "#                     y += array('d', np.log10(ctaus_m/use_m_s*m))\n",
    "#                     total_limits = 1./limits[sample_name][:,2]*br[-1]\n",
    "#                 else:\n",
    "#                     total_limits += 1./limits[sample_name][:,2]*br[-1]\n",
    "#             total_limits = np.log10(1./total_limits)\n",
    "\n",
    "        \n",
    "    \n",
    "                for ct_target in ctaus:\n",
    "        \n",
    "                    ct = ct_target/m*use_m_s #equivalent ct in original sample\n",
    "#                     m = decay_mode + '_ms'+str(mass)\n",
    "                    OLD_CTAU = old_ctau[sample_name].copy()\n",
    "\n",
    "                    ctf = int(ct)\n",
    "                    ct_list = 10**int(math.log10(ctf))\n",
    "                    if int(ct) < OLD_CTAU[0]: ct_list = [OLD_CTAU[0]]\n",
    "                    elif int(ct)>OLD_CTAU[-1]: ct_list = [OLD_CTAU[-1]]\n",
    "                    elif int(ct) in OLD_CTAU: ct_list = [int(ct)]\n",
    "                    elif len(OLD_CTAU) == 2:ct_list = OLD_CTAU.copy()\n",
    "                    else:ct_list = [ct_list,ct_list*10]\n",
    "\n",
    "\n",
    "\n",
    "                    for pr in prod:   \n",
    "                        p = decay_mode + '_' + pr\n",
    "                        signal_rate[p] = np.zeros((bins,))\n",
    "                        mc_stat_unc[p] = np.zeros((bins,))\n",
    "                        gmn[p] = np.zeros((bins,))\n",
    "                        sig_unc[p] = []\n",
    "                        for i, ct0 in enumerate(ct_list):\n",
    "                            k = 'MC_'+pr+'_'+str(m)+'_'+str(ct0)\n",
    "        #                     k = 'MC_'+p+'_'+decay_mode+'_ms'+str(m)+'_'+str(ct0)\n",
    "                            T = tree[k]\n",
    "                            if np.count_nonzero(sel_ev[k])==0:continue\n",
    "                            gLLP_ctau = T.array('gLLP_ctau')[sel_ev[k]][cond[k]]\n",
    "                            weight_ctau = weight_calc(gLLP_ctau, int(ct)/10, int(ct0)/10) # convert everything to cm\n",
    "                            gLLP_ctau = np.sum(T.array('gLLP_ctau'), axis = 1)[sel_ev[k]][cond[k]]\n",
    "                            if len(ct_list) == 1:weight_cond = gLLP_ctau >= 0\n",
    "                            else:\n",
    "                                if i == 0 : weight_cond = gLLP_ctau<int(ct_list[0]/2)\n",
    "                                else: weight_cond = gLLP_ctau>=int(ct_list[0]/2)\n",
    "                            w = weight[k]*weight_ctau\n",
    "\n",
    "                            if category == 2:\n",
    "                                a = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_DT, nhits2[k]>=N_RECHIT_CSC), weight_cond)\n",
    "                                signal_rate[p][0]+=np.sum(w[a])\n",
    "                                mc_stat_unc[p][0]+=np.sum(w[a]**2)\n",
    "                                gmn[p][0] += len(w[a])\n",
    "                                b = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_DT, nhits2[k]>=N_RECHIT_CSC), weight_cond)\n",
    "                                signal_rate[p][1]+=np.sum(w[b])\n",
    "                                mc_stat_unc[p][1]+=np.sum(w[b]**2)\n",
    "                                gmn[p][1] += len(w[b])\n",
    "                                c = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_DT, nhits2[k]<N_RECHIT_CSC), weight_cond)\n",
    "                                signal_rate[p][2]+=np.sum(w[c])\n",
    "                                mc_stat_unc[p][2]+=np.sum(w[c]**2)\n",
    "                                gmn[p][2] += len(w[c])\n",
    "                                d = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_DT, nhits2[k]<N_RECHIT_CSC), weight_cond)\n",
    "                                signal_rate[p][3]+=np.sum(w[d])\n",
    "                                mc_stat_unc[p][3]+=np.sum(w[d]**2)\n",
    "                                gmn[p][3] += len(w[d])\n",
    "                            elif datacard_version == 'v1':\n",
    "                                if category == 0:N_RECHIT_CUT = N_RECHIT_CSC\n",
    "                                else: N_RECHIT_CUT = N_RECHIT_DT\n",
    "\n",
    "                                a = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT), weight_cond)\n",
    "                                signal_rate[p][0]+=np.sum(w[a])\n",
    "                                mc_stat_unc[p][0]+=np.sum(w[a]**2)\n",
    "                                gmn[p][0] += len(w[a])\n",
    "                                b = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT), weight_cond)\n",
    "                                signal_rate[p][1]+=np.sum(w[b])\n",
    "                                mc_stat_unc[p][1]+=np.sum(w[b]**2)\n",
    "                                gmn[p][1] += len(w[b])\n",
    "                                c = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT), weight_cond)\n",
    "                                signal_rate[p][2]+=np.sum(w[c])\n",
    "                                mc_stat_unc[p][2]+=np.sum(w[c]**2)\n",
    "                                gmn[p][2] += len(w[c])\n",
    "                                d = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT), weight_cond)\n",
    "                                signal_rate[p][3]+=np.sum(w[d])\n",
    "                                mc_stat_unc[p][3]+=np.sum(w[d]**2)\n",
    "                                gmn[p][3] += len(w[d])\n",
    "\n",
    "                            else:\n",
    "                                ####\n",
    "                                # a is signal region, b has one cluster pass Nrechit cut, C has no cluster passing Nrechit cut\n",
    "                                ####\n",
    "                                if category == 0:N_RECHIT_CUT = N_RECHIT_CSC\n",
    "                                else: N_RECHIT_CUT = N_RECHIT_DT\n",
    "                                a = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT), weight_cond)\n",
    "                                signal_rate[p][0]+=np.sum(w[a])\n",
    "                                mc_stat_unc[p][0]+=np.sum(w[a]**2)\n",
    "                                gmn[p][0] += len(w[a])\n",
    "\n",
    "                                b = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT), weight_cond)\n",
    "                                d = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT), weight_cond)\n",
    "                                b = np.logical_or(b, d)\n",
    "                                signal_rate[p][1]+=np.sum(w[b])\n",
    "                                mc_stat_unc[p][1]+=np.sum(w[b]**2)\n",
    "                                gmn[p][1] += len(w[b])\n",
    "\n",
    "                                c = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT), weight_cond)\n",
    "                                signal_rate[p][2]+=np.sum(w[c])\n",
    "                                mc_stat_unc[p][2]+=np.sum(w[c]**2)\n",
    "                                gmn[p][2] += len(w[c])\n",
    "\n",
    "                            for j in range(bins):\n",
    "                                if signal_rate[p][j]<0.0:\n",
    "                                    signal_rate[p][j]=0.0\n",
    "                                    mc_stat_unc[p][j]=0.0\n",
    "                                    gmn[p][j] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "                        mc_stat_unc[p] = np.nan_to_num(signal_rate[p]/np.sqrt(mc_stat_unc[p]))**2 #gmn\n",
    "                        mc_stat_unc[p][mc_stat_unc[p] == np.inf] = 0.0\n",
    "\n",
    "                        mc_stat_unc[p] = list(mc_stat_unc[p])\n",
    "                        mc_stat_unc[p] = list(gmn[p])\n",
    "                        if p == 'others': k = k.replace(\"ggH\", \"others\")\n",
    "                        for j, ele in enumerate(sig_eff):# go through each uncertainty\n",
    "                            if j == len(sig_eff)-1: # if mc_stats\n",
    "                                sig_unc[p].append(mc_stat_unc[p])\n",
    "                            else:\n",
    "                                sig_unc[p].append(ele[k])\n",
    "\n",
    "\n",
    "                norm = np.sum(signal_rate[list(signal_rate.keys())[0]])/4\n",
    "        #         norm = 1\n",
    "                #only used for calculation, the signal yield is not changed in datacard\n",
    "\n",
    "\n",
    "                if category == 2 or datacard_version == 'v1': make_datacard_2tag(outDataCardsDir, modelName, signal_rate, norm, bkg_rate, observation, \\\n",
    "                                  bkg_unc, bkg_unc_name, sig_unc, sig_eff_name, 'a', 'category'+str(category)+'_')\n",
    "                else: make_datacard_2tag_3bins(outDataCardsDir, modelName, signal_rate, norm, bkg_rate, observation, \\\n",
    "                                      bkg_unc, bkg_unc_name, sig_unc, sig_eff_name, 'a', 'category'+str(category)+'_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
