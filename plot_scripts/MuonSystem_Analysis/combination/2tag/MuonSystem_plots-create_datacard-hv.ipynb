{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/02\n",
      "3.6.8 (default, Aug  7 2019, 17:28:10) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]\n"
     ]
    }
   ],
   "source": [
    "# create datacard for 2 tag\n",
    "\n",
    "import ROOT as rt\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import collections\n",
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "# import ot\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import scipy\n",
    "import awkward\n",
    "import numpy as np\n",
    "import time\n",
    "import numba\n",
    "from numba import jit\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append('/storage/af/user/christiw/gpu/christiw/llp/delayed_jet_analyzer/lib/')\n",
    "sys.path.append('/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_9_4_4/src/llp_analyzer/python/')\n",
    "from helper import make_datacard_2sig, make_datacard_2tag, weight_calc\n",
    "from histo_utilities import create_TH1D, create_TH2D, std_color_list, create_TGraph, make_ratio_plot\n",
    "from helper_functions import deltaR, deltaPhi\n",
    "\n",
    "import CMS_lumi, tdrstyle\n",
    "tdrstyle.setTDRStyle()\n",
    "CMS_lumi.writeExtraText = 0\n",
    "\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ntuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath =OrderedDict()\n",
    "tree = OrderedDict()\n",
    "\n",
    "scale = ['xiO_2p5_xiL_1', 'xiO_2p5_xiL_2p5', 'xiO_1_xiL_1']\n",
    "prod = ['ggH']\n",
    "portal = {\n",
    "'higgs': [4, 5, 10, 15, 20],\n",
    "'gluon': [3, 5, 10, 15, 20],\n",
    "'vector':[2, 5, 10, 15, 20],\n",
    "'photon':[2, 5, 10, 15, 20],\n",
    "'darkphoton':[2, 5, 10, 15, 20],\n",
    "}\n",
    "# old_ctau = [500, 1000, 5000, 10000]\n",
    "old_ctau = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "\n",
    "\n",
    "ntupler_version = 'V1p17/'\n",
    "analyzer_version = 'v2/v163/'\n",
    "\n",
    "\n",
    "mc_path = {}\n",
    "mc_path= '/storage/af/group/phys_exotica/delayedjets/displacedJetMuonAnalyzer/csc/'+ntupler_version+'/MC_all/'+analyzer_version+'/normalized/'\n",
    "for x in scale:\n",
    "    for p in portal.keys():\n",
    "        for m in portal[p]:\n",
    "            for ct in old_ctau:\n",
    "#                 if not x == 'xiO_2p5_xiL_2p5':continue\n",
    "#                 if not p == 'darkphoton':continue\n",
    "#                 if not m == 5:continue\n",
    "#                 if not ct == 500:continue\n",
    "                key = 'HV_params_'+p+'_m_'+str(m) + '_ctau_'+str(ct)+'mm_'+x\n",
    "                fpath[key] = mc_path+key+'.root'\n",
    "                print(fpath[key])\n",
    "\n",
    "# fpath['old']= '/storage/af/group/phys_exotica/delayedjets/displacedJetMuonAnalyzer/csc/V1p17//MC_Summer16/v1/v163//normalized/HV_params_darkphoton_m_5_ctau_500mm_xiO_2p5_xiL_2p5.root'\n",
    "# fpath['new']= '/storage/af/group/phys_exotica/delayedjets/displacedJetMuonAnalyzer/csc/V1p17//MC_Summer16/v2/v163//normalized/HiddenValley_darkphoton_m_5_ctau_500_xiO_2p5_xiL_2p5.root'\n",
    "\n",
    "data_path = '/storage/af/group/phys_exotica/delayedjets/displacedJetMuonAnalyzer/csc/V1p17/Data2018/v5/v170/normalized/'\n",
    "\n",
    "# fpath['data'] = data_path + 'Run2_displacedJetMuonNtupler_V1p17_Data2016_Data2017_Data2018-HighMET_goodLumi.root'\n",
    "\n",
    "NEvents = {}\n",
    "NEvents_genweight = {}\n",
    "for k,v in fpath.items():\n",
    "    root_dir = uproot.open(v) \n",
    "    if not root_dir: \n",
    "        print(k, \"zombie\")\n",
    "        continue\n",
    "    tree[k] = root_dir['MuonSystem']\n",
    "    NEvents[k] = root_dir['NEvents'][1]\n",
    "    \n",
    "    w = tree[k][\"weight\"].array()\n",
    "    if not 'data' in k: \n",
    "        print(k, root_dir['NEvents']._fEntries)\n",
    "        \n",
    "\n",
    "\n",
    "root_dir = uproot.open('/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_9_4_4/src/llp_analyzer/data/HiggsPtWeights/ZHToggZH_HiggsPtReweight.root') \n",
    "h_reweight = root_dir['higgsPthiggsEta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nCsc with different hit vetoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEVENTS = 2000000\n",
    "XSEC = 48.58 #pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "jetPt_cut = 50\n",
    "\n",
    "weight = {}\n",
    "nhits1 = {}\n",
    "nhits2 = {}\n",
    "sel_ev = {}\n",
    "cond = {}\n",
    "cond_dtnoise_2016 = {}\n",
    "ggZH_weight = {}\n",
    "higgsEta = {}\n",
    "higgsPt = {}\n",
    "\n",
    "nCosmic = {}\n",
    "deltaEta = {}\n",
    "deltaRCluster = {}\n",
    "jetPt = {}\n",
    "jetTightPassId = {}\n",
    "runNum = {}\n",
    "evtNum = {}\n",
    "lumiNum = {}\n",
    "sel_ev_dtnoise_2016 = {}\n",
    "nhits1_dtnoise_2016 = {}\n",
    "nhits2_dtnoise_2016 = {}\n",
    "cluster_index = ''\n",
    "addNoiseFlag = 1\n",
    "# 0: 2 CSC; 1: 2DT; 2: csc+dt\n",
    "category = 0\n",
    "\n",
    "\n",
    "for k in list(tree.keys()):\n",
    "#     if not 'ggH' in k:continue\n",
    "    if   'data' in k:continue\n",
    "########### SELECTION: CLUSTERS ############\n",
    "#     print(k)\n",
    "#     if not 'HV_params_higgs_m_10' in k and not 'data' in k:continue\n",
    "    if 'data' in k: T = tree['data']\n",
    "    else: T = tree[k]\n",
    "\n",
    "    sel_csccluster = T.array('cscRechitCluster' + cluster_index + 'TimeSpreadWeightedAll')<20\n",
    "    sel_csccluster = np.logical_and(sel_csccluster, np.abs(T.array('cscRechitCluster' + cluster_index + 'MetEENoise_dPhi'))<1.2)\n",
    "    sel_csccluster = np.logical_and(sel_csccluster, T.array('cscRechitCluster' + cluster_index + 'JetVetoPt')<30)\n",
    "    sel_csccluster = np.logical_and(sel_csccluster, T.array('cscRechitCluster' + cluster_index + 'Me11Ratio')<1)\n",
    "    sel_csccluster = np.logical_and(sel_csccluster, np.logical_not(np.logical_and(T.array('cscRechitClusterMuonVetoPt') >= 30, T.array('cscRechitClusterMuonVetoGlobal'))))\n",
    "    \n",
    "    if 'data' in k: \n",
    "        sel_csccluster = np.logical_and(sel_csccluster, np.logical_and(T.array('cscRechitCluster' + cluster_index + 'TimeWeighted')< 12.5, \\\n",
    "                                                                         T.array('cscRechitCluster' + cluster_index + 'TimeWeighted') > -5))\n",
    "    else: \n",
    "        sel_csccluster = np.logical_and(sel_csccluster, np.logical_and(T.array('cscRechitCluster' + cluster_index + 'TimeWeighted')+0.66 < 12.5, \\\n",
    "                                                                         T.array('cscRechitCluster' + cluster_index + 'TimeWeighted')+0.66 > -5))\n",
    "        \n",
    "    sel_dtcluster = np.abs(T.array('dtRechitClusterMetEENoise_dPhi')) < 1\n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_not(np.logical_and(T.array('dtRechitClusterMuonVetoPt') >= 10, T.array('dtRechitClusterMuonVetoLooseId'))))\n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.abs(T.array('dtRechitClusterJetVetoPt')) < 50)\n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_not(np.logical_and(T.array('dtRechitClusterMaxStation')==1, T.array('dtRechitClusterMaxStationRatio')>0.9)))\n",
    "\n",
    "    \n",
    "    cut = 5\n",
    "    station = (T.array('dtRechitClusterNSegmentStation1')>cut).astype(int)+(T.array('dtRechitClusterNSegmentStation2')>cut).astype(int)\\\n",
    "+(T.array('dtRechitClusterNSegmentStation3')>cut).astype(int)+(T.array('dtRechitClusterNSegmentStation4')>cut).astype(int)\n",
    "\n",
    "    max_station = np.maximum(np.maximum(np.maximum(T.array('dtRechitClusterNSegmentStation1'), T.array('dtRechitClusterNSegmentStation2')), T.array('dtRechitClusterNSegmentStation3')), T.array('dtRechitClusterNSegmentStation4'))\n",
    "    min_station = np.minimum(np.minimum(np.minimum(T.array('dtRechitClusterNSegmentStation1'), T.array('dtRechitClusterNSegmentStation2')), T.array('dtRechitClusterNSegmentStation3')), T.array('dtRechitClusterNSegmentStation4'))\n",
    "\n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_or(station<4, 1.0*min_station/max_station<0.4)) #remove if both clusters are 4 stations\n",
    "    if addNoiseFlag and not 'data' in k: \n",
    "        sel_dtcluster = np.logical_and(sel_dtcluster, (T.array('dtRechitClusterSize')+T.array('dtRechitClusterNoiseHit')) >= 50)\n",
    "    else: sel_dtcluster = np.logical_and(sel_dtcluster, T.array('dtRechitClusterSize') >= 50)\n",
    "\n",
    "\n",
    "\n",
    "    if k in 'data': \n",
    "        sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_not(T.array('dtRechitClusterNoiseVeto')))\n",
    "        sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_not(T.array('dtRechitClusterOverlap')))\n",
    "    ###################### cosmic muon veto #############\n",
    "    sel_cosmic = np.logical_and(T.array('dtRechitClusterNOppositeSegStation1')>0, T.array('dtRechitClusterNOppositeSegStation2')>0)\n",
    "    sel_cosmic = np.logical_and(sel_cosmic, T.array('dtRechitClusterNOppositeSegStation3')>0)\n",
    "    sel_cosmic = np.logical_and(sel_cosmic, T.array('dtRechitClusterNOppositeSegStation4')>0)\n",
    "    sel_cosmic = np.logical_and(sel_cosmic, T.array('dtRechitClusterNOppositeSegStation1')+T.array('dtRechitClusterNOppositeSegStation2')+\\\n",
    "                               T.array('dtRechitClusterNOppositeSegStation3')+T.array('dtRechitClusterNOppositeSegStation4')>=6)\n",
    "    nstation = (T.array('dtRechitClusterNSegmentStation1')>1).astype(int)+(T.array('dtRechitClusterNSegmentStation2')>1).astype(int)\\\n",
    "    +(T.array('dtRechitClusterNSegmentStation3')>1).astype(int)+(T.array('dtRechitClusterNSegmentStation4')>1).astype(int)\n",
    "    \n",
    "    sel_dtcluster = np.logical_and(sel_dtcluster, np.logical_not(np.logical_and(nstation>=3, sel_cosmic)))\n",
    "\n",
    "    noise_2016 = np.logical_and(T.array('dtRechitClusterMaxStation') == 2, T.array('dtRechitClusterWheel') == 1)\n",
    "    noise_2016 = np.logical_and(noise_2016, T.array('dtRechitClusterPhi') > math.pi/12)\n",
    "    noise_2016 = np.logical_and(noise_2016, T.array('dtRechitClusterPhi') < math.pi/4)\n",
    "    sel_dtcluster_noise_2016 = np.logical_and(sel_dtcluster, np.logical_not(noise_2016))\n",
    "\n",
    "\n",
    "########### SELECTION: JETS ############\n",
    "    \n",
    "    sel_jet = np.logical_and(T.array('jetPt') > 30, np.abs(T.array('jetEta')) < 2.4 )\n",
    "    sel_jet = np.logical_and(T.array('jetTightPassId'), sel_jet)\n",
    "            \n",
    "########### SELECTION: NOISE IN DT ############\n",
    "    \n",
    "    spike = np.logical_and( T.array('nDTRechitsSector')[:,0,0,7]>50,  T.array('nDTRechitsSector')[:,0,0,7]+T.array('nDTRechitsSector')[:,0,0,8]+T.array('nDTRechitsSector')[:,0,0,9]>120)\n",
    "    spike = np.logical_and(spike, T.array('nDTRechitsSector')[:,0,0,8]>25)\n",
    "    spike = np.logical_and(spike, T.array('nDTRechitsSector')[:,0,0,9]>10)\n",
    "\n",
    "    \n",
    "    \n",
    "########### SELECTION: EVENTS ############\n",
    "\n",
    "    sel_ev[k] = T.array('METNoMuTrigger')\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k] ,T.array('metEENoise') >= 200)\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k] , sel_jet.sum()>=1)\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k], (T.array('nDtRings')+T.array('nCscRings'))<10)\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k],T.array('Flag2_all'))\n",
    "    sel_ev[k] = np.logical_and(sel_ev[k] , np.logical_not(spike))\n",
    "########### BRANCHES ############\n",
    "                            \n",
    "    if category == 0:\n",
    "        sel_ev[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum()== 2)\n",
    "        sel_ev[k]  = np.logical_and(sel_ev[k],sel_dtcluster.sum()== 0)\n",
    "        cond[k] = deltaR(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,0], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,0],\\\n",
    "                        T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,1], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,1])<2\n",
    "        \n",
    "       \n",
    "        nhits1[k] =  T.array('cscRechitCluster' + cluster_index + 'Size')[sel_csccluster][sel_ev[k]][cond[k]][:,0]\n",
    "        nhits2[k] =  T.array('cscRechitCluster' + cluster_index + 'Size')[sel_csccluster][sel_ev[k]][cond[k]][:,1]\n",
    "#         deltaRCluster[k] = deltaR(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,0], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,0],\\\n",
    "#                         T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,1], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,1])\n",
    "    elif category == 1:\n",
    "        \n",
    "        \n",
    "        # data\n",
    "        if k == 'data':\n",
    "            sel_ev[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum()== 0)\n",
    "            sel_ev[k]  = np.logical_and(sel_ev[k],sel_dtcluster.sum()== 2)\n",
    "            nhits1[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][:,0]\n",
    "            nhits2[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][:,1]\n",
    "        else:\n",
    "            # reweight to 1.5/fb, for events with dt noise\n",
    "            sel_ev_dtnoise_2016[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum() == 0)\n",
    "            sel_ev_dtnoise_2016[k]  = np.logical_and(sel_ev_dtnoise_2016[k],sel_dtcluster_noise_2016.sum() == 2)\n",
    "\n",
    "            if addNoiseFlag and not 'data' in k:\n",
    "                nhits1_dtnoise_2016[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster_noise_2016][sel_ev_dtnoise_2016[k]][:,0]\n",
    "                nhits2_dtnoise_2016[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster_noise_2016][sel_ev_dtnoise_2016[k]][:,1]\n",
    "            else:\n",
    "                nhits1_dtnoise_2016[k] =  T.array('dtRechitClusterSize')[sel_dtcluster_noise_2016][sel_ev_dtnoise_2016[k]][:,0]\n",
    "                nhits2_dtnoise_2016[k] =  T.array('dtRechitClusterSize')[sel_dtcluster_noise_2016][sel_ev_dtnoise_2016[k]][:,1]\n",
    "\n",
    "            # normal condition\n",
    "            sel_ev[k]  = np.logical_and(sel_ev[k],sel_dtcluster.sum()== 2)\n",
    "            sel_ev[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum()== 0)\n",
    "            if k == 'data':\n",
    "                sel_ev[k] = np.logical_and()\n",
    "\n",
    "            if addNoiseFlag and not 'data' in k:\n",
    "                nhits1[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster][sel_ev[k]][:,0]\n",
    "                nhits2[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster][sel_ev[k]][:,1]\n",
    "            else:\n",
    "                nhits1[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][:,0]\n",
    "                nhits2[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][:,1]\n",
    "        \n",
    "#             nhits1[k] = np.hstack((nhits1[k], nhits1_dtnoise_2016[k]))\n",
    "#             nhits2[k] = np.hstack((nhits2[k], nhits2_dtnoise_2016[k]))\n",
    "\n",
    "    elif category == 2:\n",
    "        # data\n",
    "        if k == 'data':\n",
    "            sel_ev[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum()== 1)\n",
    "            sel_ev[k]  = np.logical_and(sel_ev[k],sel_dtcluster.sum()== 1)\n",
    "            cond[k] = deltaR(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,0], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,0],\\\n",
    "                            T.array('dtRechitCluster' + cluster_index + 'Eta')[sel_dtcluster][sel_ev[k]][:,0], T.array('dtRechitCluster' + cluster_index + 'Phi')[sel_dtcluster][sel_ev[k]][:,0])<2.5\n",
    "            nhits1[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][cond[k]][:,0]\n",
    "            nhits2[k] =  T.array('cscRechitClusterSize')[sel_csccluster][sel_ev[k]][cond[k]][:,0]\n",
    "        else:\n",
    "            sel_ev_dtnoise_2016[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum() == 1)\n",
    "            sel_ev_dtnoise_2016[k]  = np.logical_and(sel_ev_dtnoise_2016[k],sel_dtcluster_noise_2016.sum() == 1)\n",
    "\n",
    "            sel_ev[k]  = np.logical_and(sel_ev[k],sel_csccluster.sum() == 1)\n",
    "            sel_ev[k]  = np.logical_and(sel_ev[k],sel_dtcluster.sum() == 1)\n",
    "\n",
    "\n",
    "            cond[k] = deltaR(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev[k]][:,0], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev[k]][:,0],\\\n",
    "                            T.array('dtRechitCluster' + cluster_index + 'Eta')[sel_dtcluster][sel_ev[k]][:,0], T.array('dtRechitCluster' + cluster_index + 'Phi')[sel_dtcluster][sel_ev[k]][:,0])<2.5\n",
    "            if addNoiseFlag and not 'data' in k:\n",
    "                nhits1[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster][sel_ev[k]][cond[k]][:,0]\n",
    "            else:\n",
    "                nhits1[k] =  T.array('dtRechitClusterSize')[sel_dtcluster][sel_ev[k]][cond[k]][:,0]\n",
    "            nhits2[k] =  T.array('cscRechitCluster' + cluster_index + 'Size')[sel_csccluster][sel_ev[k]][cond[k]][:,0]\n",
    "\n",
    "            # reweight to 1.5/fb, for events with dt noise\n",
    "            cond_dtnoise_2016[k] = deltaR(T.array('cscRechitCluster' + cluster_index + 'Eta')[sel_csccluster][sel_ev_dtnoise_2016[k]][:,0], T.array('cscRechitCluster' + cluster_index + 'Phi')[sel_csccluster][sel_ev_dtnoise_2016[k]][:,0],\\\n",
    "                            T.array('dtRechitCluster' + cluster_index + 'Eta')[sel_dtcluster_noise_2016][sel_ev_dtnoise_2016[k]][:,0], T.array('dtRechitCluster' + cluster_index + 'Phi')[sel_dtcluster_noise_2016][sel_ev_dtnoise_2016[k]][:,0])<2.5\n",
    "            if addNoiseFlag and not 'data' in k:\n",
    "                nhits1_dtnoise_2016[k] =  (T.array('dtRechitClusterNoiseHit')+T.array('dtRechitClusterSize'))[sel_dtcluster_noise_2016][sel_ev_dtnoise_2016[k]][cond_dtnoise_2016[k]][:,0]\n",
    "            else:\n",
    "                nhits1_dtnoise_2016[k] =  T.array('dtRechitClusterSize')[sel_dtcluster_noise_2016][sel_ev_dtnoise_2016[k]][cond_dtnoise_2016[k]][:,0]\n",
    "            nhits2_dtnoise_2016[k] =  T.array('cscRechitCluster' + cluster_index + 'Size')[sel_csccluster][sel_ev_dtnoise_2016[k]][cond_dtnoise_2016[k]][:,0]\n",
    "#             nhits1[k] = np.hstack((nhits1[k], nhits1_dtnoise_2016[k]))\n",
    "#             nhits2[k] = np.hstack((nhits2[k], nhits2_dtnoise_2016[k]))\n",
    "    else:\n",
    "        assert(False)\n",
    "    if not k in 'data':\n",
    "        if category == 0:\n",
    "            weight[k] = (T.array('pileupWeight')*T.array('higgsPtWeight')*T.array('metSF'))*XSEC/NEVENTS\n",
    "            weight[k][T.array('MC_condition')==2016] *= 35920 \n",
    "            weight[k][T.array('MC_condition')==2017] *= 41530 \n",
    "            weight[k][T.array('MC_condition')==2018] *= 59740 \n",
    "            weight[k] = weight[k][sel_ev[k]][cond[k]]\n",
    "        elif category == 1:\n",
    "            \n",
    "            weight[k] = (T.array('pileupWeight')*T.array('higgsPtWeight')*T.array('metSF'))*XSEC/NEVENTS\n",
    "            weight[k][T.array('MC_condition')==2016] *= 35920 \n",
    "            weight[k][T.array('MC_condition')==2017] *= 41530 \n",
    "            weight[k][T.array('MC_condition')==2018] *= 59740 \n",
    "            weight[k] = weight[k][sel_ev[k]]\n",
    "            weight[k] *= (137-1.5)/137\n",
    "\n",
    "#             weight_temp =  (T.array('pileupWeight')*T.array('higgsPtWeight')*T.array('metSF'))*XSEC/NEVENTS\n",
    "#             weight_temp[T.array('MC_condition')==2016] *= 35920 \n",
    "#             weight_temp[T.array('MC_condition')==2017] *= 41530 \n",
    "#             weight_temp[T.array('MC_condition')==2018] *= 59740 \n",
    "#             weight_temp = weight_temp[sel_ev_dtnoise_2016[k]]\n",
    "#             weight[k] = np.hstack((weight[k], weight_temp * 1.5/137))\n",
    "        else:\n",
    "            weight[k] = (T.array('pileupWeight')*T.array('higgsPtWeight')*T.array('metSF'))*XSEC/NEVENTS\n",
    "            weight[k][T.array('MC_condition')==2016] *= 35920 \n",
    "            weight[k][T.array('MC_condition')==2017] *= 41530 \n",
    "            weight[k][T.array('MC_condition')==2018] *= 59740 \n",
    "            weight[k] = weight[k][sel_ev[k]][cond[k]]\n",
    "            weight[k] *= (137-1.5)/137\n",
    "\n",
    "#             weight_temp =  (T.array('pileupWeight')*T.array('higgsPtWeight')*T.array('metSF'))*XSEC/NEVENTS\n",
    "#             weight_temp[T.array('MC_condition')==2016] *= 35920 \n",
    "#             weight_temp[T.array('MC_condition')==2017] *= 41530 \n",
    "#             weight_temp[T.array('MC_condition')==2018] *= 59740 \n",
    "#             weight_temp = weight_temp[sel_ev_dtnoise_2016[k]][cond_dtnoise_2016[k]]\n",
    "#             weight[k] = np.hstack((weight[k], weight_temp * 1.5/137))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "path_txt = '/storage/af/user/christiw/gpu/christiw/llp/delayed_jet_analyzer/plot_scripts/MuonSystem_Analysis/combination/2tag/hv_stat_unc/v2/'\n",
    "with open(path_txt + 'category' + str(category)+'.txt', 'w') as f:\n",
    "    f.write(\"sample name \\t number of events in SR  \\t relative uncertainty \\n\")\n",
    "    for k in weight.keys():\n",
    "#         if not k == 'HV_params_darkphoton_m_5_ctau_500mm_xiO_2p5_xiL_2p5':continue\n",
    "        if category == 0:\n",
    "            N_RECHIT_CUT_1 = 100\n",
    "            N_RECHIT_CUT_2 = 100\n",
    "        elif category == 1:\n",
    "            N_RECHIT_CUT_1 = 80\n",
    "            N_RECHIT_CUT_2 = 80\n",
    "        else:\n",
    "            N_RECHIT_CUT_1 = 80\n",
    "            N_RECHIT_CUT_2 = 100\n",
    "        cond_a = np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2)#bin A\n",
    "        if np.count_nonzero(cond_a) > 0 and np.sum(weight[k][cond_a]*weight[k][cond_a])**0.5/np.sum(weight[k][cond_a])<0.1:continue\n",
    "\n",
    "\n",
    "\n",
    "        f.write(k + '\\t' + str(len(weight[k][cond_a])) + '\\t' + str(np.sum(weight[k][cond_a]**2)**0.5/np.sum(weight[k][cond_a])) + '\\n')\n",
    "\n",
    "#         print(k + '\\t' + str(len(weight[k][cond_a])) + '\\t' + str(np.sum(weight[k][cond_a]**2)**0.5/np.sum(weight[k][cond_a])) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category 0\n",
      "18 7 10\n",
      "17.174353249225614 8.254761341781615 9.798154972000123\n"
     ]
    }
   ],
   "source": [
    "print('category', category)\n",
    "k = 'data'\n",
    "k = 'HV_params_darkphoton_m_5_ctau_500mm_xiO_2p5_xiL_2p5'\n",
    "if category == 0:\n",
    "    N_RECHIT_CUT_1 = 100\n",
    "    N_RECHIT_CUT_2 = 100\n",
    "elif category == 1:\n",
    "    N_RECHIT_CUT_1 = 80\n",
    "    N_RECHIT_CUT_2 = 80\n",
    "else:\n",
    "    N_RECHIT_CUT_1 = 80\n",
    "    N_RECHIT_CUT_2 = 100\n",
    "    \n",
    "\n",
    "cond_a = np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2)#bin A\n",
    "cond_b =  np.logical_and(nhits1[k]<N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2)\n",
    "if category == 2:cond_d = np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]<N_RECHIT_CUT_2)#bin A\n",
    "else:cond_b = np.logical_or(cond_b, np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]<N_RECHIT_CUT_2))#bin A\n",
    "cond_c = np.logical_and(nhits1[k]<N_RECHIT_CUT_1, nhits2[k]<N_RECHIT_CUT_2)#bin \n",
    "# cond = np.abs(deltaRCluster[k])<2000\n",
    "# cond_a = np.logical_and(cond, cond_a)\n",
    "# cond_b = np.logical_and(cond, cond_b)\n",
    "# cond_c = np.logical_and(cond, cond_c)\n",
    "# if category == 2:cond_d = np.logical_and(cond, cond_d)\n",
    "\n",
    "if category == 2: \n",
    "    print(np.count_nonzero(cond_b), np.count_nonzero(cond_c), np.count_nonzero(cond_d))\n",
    "\n",
    "    print(np.sum(weight[k][cond_a]), np.sum(weight[k][cond_b]), np.sum(weight[k][cond_c]), np.sum(weight[k][cond_d]))\n",
    "\n",
    "else:\n",
    "    print(np.count_nonzero(cond_b), np.count_nonzero(cond_c), np.count_nonzero(cond_a))\n",
    "    print(np.sum(weight[k][cond_b]), np.sum(weight[k][cond_c]), np.sum(weight[k][cond_a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 80 80\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 809 but corresponding boolean dimension is 406",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-37dfce2f9b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcond_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnhits1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mN_RECHIT_CUT_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhits2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mN_RECHIT_CUT_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#bin A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#             print(k, np.sum(w))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 809 but corresponding boolean dimension is 406"
     ]
    }
   ],
   "source": [
    "BR = 0.01\n",
    "file1 = open(\"category\"+str(category)+\".txt\",\"w\")\n",
    "file1.write(\"sample nane \\t relative uncertainty \\t signal yield (BR = 1) \\t Nevents \\n\")\n",
    "for N_RECHIT_CUT_1 in np.arange(60,220,1):\n",
    "    for N_RECHIT_CUT_2 in np.arange(60,220,1):\n",
    "        if category < 2 and not N_RECHIT_CUT_1 == N_RECHIT_CUT_2:continue\n",
    "        if category == 0 and not N_RECHIT_CUT_1 == N_RECHIT_CUT_2 == 100:continue\n",
    "        if category == 1 and not N_RECHIT_CUT_1 == 80:continue\n",
    "        if category == 2 and (not N_RECHIT_CUT_1 == 80 or not N_RECHIT_CUT_2 == 100):continue\n",
    "        print(category, N_RECHIT_CUT_1, N_RECHIT_CUT_2)\n",
    "      \n",
    "        for k in weight.keys():\n",
    "            if k == 'data':continue\n",
    "            if np.count_nonzero(sel_ev[k]) == 0: continue\n",
    "            # signal region\n",
    "            cond_d = np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2)#bin A\n",
    "            if np.count_nonzero(cond_d) == 0: continue\n",
    "            if category == 0:w = weight[k][cond_d]\n",
    "            else: w = weight[k][cond_d]\n",
    "#             print(k, np.sum(w))\n",
    "\n",
    "#             if np.sum(weight[k][cond_d]*weight[k][cond_d])**0.5/np.sum(weight[k][cond_d])>0.2:\n",
    "#                 print(k, '\\t',np.sum(weight[k][cond_d]*weight[k][cond_d])**0.5/np.sum(weight[k][cond_d]))\n",
    "#     , weight[k][cond_d].min(), weight[k][cond_d].max(), np.mean(weight[k][cond_d]))\n",
    "            if np.sum(weight[k][cond_d]*weight[k][cond_d])**0.5/np.sum(weight[k][cond_d])<0.1:continue\n",
    "            file1.write(k+'\\t'+str(np.sum(weight[k][cond_d]*weight[k][cond_d])**0.5/np.sum(weight[k][cond_d]))+'\\t'+str(np.sum(weight[k][cond_d]))+'\\t'+str(NEvents[k])+'\\n')\n",
    "file1.close() #to change file access modes\n",
    "#             print(k, '\\t', NEvents[k])\n",
    "#             print(k, '\\t',np.sum(weight[k][cond_d]), '\\t',np.sum(weight[k][cond_d]*weight[k][cond_d])**0.5, np.sum(weight[k][cond_d]*weight[k][cond_d])**0.5/np.sum(weight[k][cond_d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# signal yield summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-61-a58a9750088c>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-a58a9750088c>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    'ggH'ds = ['ggH', 'VBFH_H','WH','ZH','ttH_H','ggZH']\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ctaus = [ '100', '1000','10000','100000' ]\n",
    "\n",
    "\n",
    "BR = 0.01\n",
    "\n",
    "for N_RECHIT_CUT_1 in np.arange(60,220,1):\n",
    "    for N_RECHIT_CUT_2 in np.arange(60,220,1):\n",
    "        if category < 2 and not N_RECHIT_CUT_1 == N_RECHIT_CUT_2:continue\n",
    "        if category == 0 and not N_RECHIT_CUT_1 == N_RECHIT_CUT_2 == 100:continue\n",
    "        if category == 1 and not N_RECHIT_CUT_1 == 80:continue\n",
    "        if category == 2 and (not N_RECHIT_CUT_1 == 80 or not N_RECHIT_CUT_2 == 100):continue\n",
    "        print(category, N_RECHIT_CUT_1, N_RECHIT_CUT_2)\n",
    "        \n",
    "        total_sig = 0\n",
    "        for m in mass:\n",
    "    #         if not m == 55:continue\n",
    "            signal_rate = []\n",
    "            unc_rate = []\n",
    "            signal_unc = []\n",
    "            for ct in ctaus:   \n",
    "    #             if not ct == '1000':continue\n",
    "                signal = 0\n",
    "                sig_unc = 0\n",
    "                shape_unc_temp = 0\n",
    "                ctf = int(ct)\n",
    "                if ctf < OLD_CTAU[0]:\n",
    "                    old_ctau_temp = np.array([OLD_CTAU[0]])\n",
    "\n",
    "\n",
    "                weight_sum = 0\n",
    "                weight_len = 0\n",
    "                for j,ct0 in enumerate(old_ctau_temp):\n",
    "                    'ggH'ds = ['ggH', 'VBFH_H','WH','ZH','ttH_H','ggZH']\n",
    "                    'ggH'ds = ['ggH']\n",
    "                    for p in 'ggH'ds:\n",
    "\n",
    "#                         k = 'MC_'+p+'_'+str(m)+'_'+str(ct0)+''\n",
    "\n",
    "                        k = 'MC_ggH_STodd_ms3p0_100'\n",
    "                        k = 'MC_ggH_STodd_ms15_1000'\n",
    "                        if np.count_nonzero(sel_ev[k]) == 0: continue\n",
    "\n",
    "        \n",
    "                        # signal region\n",
    "                        cond_d = np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2)#bin A\n",
    "\n",
    "                        signal += np.sum(w[cond_d])\n",
    "                        sig_unc +=np.sum(w[cond_d]*w[cond_d])\n",
    "\n",
    "\n",
    "                signal_rate.append(signal)\n",
    "                signal_unc.append(sig_unc**0.5)\n",
    "                unc_rate.append(shape_unc_temp)\n",
    "            signal_rate = np.array(signal_rate)\n",
    "            unc_rate = np.array(unc_rate)\n",
    "            signal_unc = np.array(signal_unc)\n",
    "#             print(m,\":\", [num for num in signal_rate*BR*signal_correction])\n",
    "#             print(m,\":\", [num for num in signal_unc*BR*signal_correction])\n",
    "            print(m, '\\t', '\\t'.join(map(str, [round(num,5) for num in signal_rate*BR])))\n",
    "            total_sig += np.sum(signal_rate*BR)\n",
    "        print(total_sig)\n",
    "#             print(m, '\\t', '\\t'.join(map(str, [round(num,5) for num in signal_rate*BR*signal_correction])))\n",
    "#             print(m, 'GeV &', ' & '.join(map(str, [round(num,1) for num in signal_rate*BR*signal_correction])), '\\\\\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category 1\n",
      "1 3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datacard_version  = 'v1' # ABCD for all categories\n",
    "datacard_version  = 'v2' # ABCD for CSC-DT and 3 bins for CSC-CSC and DT-DT\n",
    "datacard_version  = 'v3' # signal uncertainties and corrections applied\n",
    "datacard_version  = 'v4' # global muon veto for csc and loose id muon veto for dt\n",
    "datacard_version  = 'v5' # global muon veto + eta cut for csc and loose id muon veto for dt\n",
    "datacard_version  = 'v6' # global muon veto for csc and loose id muon veto + station veto +tight ID jet veto for dt\n",
    "\n",
    "datacard_version  = 'v7' # deltaEta cut 2.5 (DT-CSC) and 1.5 (CSC-CSC)\n",
    "datacard_version  = 'v8' # deltaEta cut 2 (DT-CSC) and 1.5 (CSC-CSC)\n",
    "\n",
    "datacard_version  = 'v9' # deltaR cut for CSC and DT\n",
    "datacard_version  = 'v10' # change ABCD rateParam to the simple case\n",
    "datacard_version  = 'v11' # change ABCD rateParam to the simple case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# signal uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: 1\n",
      "bins: 3\n"
     ]
    }
   ],
   "source": [
    "cscCluster_mc = {}\n",
    "cscCluster_data = {}\n",
    "time_spread_mc = {}\n",
    "time_spread_data = {}\n",
    "dtCluster = {}\n",
    "higgsPtWeight = {}\n",
    "JES = {}\n",
    "pileup = {}\n",
    "# ctau_reweight = {}\n",
    "mc_stats = {}\n",
    "scaling = {}\n",
    "lumi = {}\n",
    "time_spread = {}\n",
    "time = {}\n",
    "theory = {}\n",
    "pdf = {}\n",
    "readout = {}\n",
    "ggH_higgsPt , VBFH_higgsPt , WH_higgsPt , ZH_higgsPt , ttH_higgsPt , ggZH_higgsPt =({} for i in range(6)) \n",
    "ggH_xsec , VBFH_xsec , WH_xsec , ZH_xsec , ttH_xsec , ggZH_xsec =({} for i in range(6)) \n",
    "ggH_pdf , VBFH_pdf , WH_pdf , ZH_pdf , ttH_pdf , ggZH_pdf =({} for i in range(6)) \n",
    "Nrechit_weight_file = '/storage/user/christiw/login-1/christiw/LLP/CMSSW_9_4_4/src/llp_analyzer/data/Uncertainties/Nrechit_weight.txt'\n",
    "\n",
    "cut_based_unc = [0.0921, 0.0852] # muon Eta reweighted\n",
    "clustering_unc = [0.1506, 0.1047]\n",
    "if category == 2 or datacard_version == 'v1': bins = 4\n",
    "else: bins = 3\n",
    "print('category: '+ str(category))\n",
    "print('bins: '+ str(bins))\n",
    "\n",
    "for x in scale:\n",
    "    for p in portal.keys():\n",
    "        for m in portal[p]:\n",
    "            for ct in old_ctau:\n",
    "                k = 'HV_params_'+p+'_m_'+str(m) + '_ctau_'+str(ct)+'mm_'+x\n",
    "                key = 'HV_params_'+p+'_m_'+str(m) + '_ctau_'+str(ct)+'mm_'+x\n",
    "\n",
    "\n",
    "                # CSC hits\n",
    "                if category == 0:\n",
    "                    cscCluster_mc[key] = [0.02] * bins\n",
    "                    cscCluster_data[key] = [0.012] * bins\n",
    "                    time_spread_mc[key] = [0.037*2] * bins\n",
    "                    time_spread_data[key] = [0.039*2] * bins\n",
    "                    dtCluster[key] = [0.0] * bins\n",
    "                    JES[key] = [0.0351,0.0573, 0.0470] \n",
    "                    pileup[key] = [0.0102,0.0144,0.0183]\n",
    "\n",
    "                elif category == 1:\n",
    "                    cscCluster_mc[key] = [0.0] * bins\n",
    "                    cscCluster_data[key] = [0.0] * bins\n",
    "                    time_spread_mc[key] = [0.0] * bins\n",
    "                    time_spread_data[key] = [0.0] * bins\n",
    "                    dtCluster[key] = [0.032] * bins\n",
    "                    JES[key] = [0.027, 0.036,0.079]\n",
    "                    pileup[key] = [0.013, 0.0152, 0.0783]\n",
    "                else: \n",
    "                    cscCluster_mc[key] = [0.013] * bins\n",
    "                    cscCluster_data[key] = [0.017] * bins\n",
    "                    time_spread_mc[key] = [0.037] * bins\n",
    "                    time_spread_data[key] = [0.039] * bins\n",
    "                    dtCluster[key] = [0.011] * bins\n",
    "                    JES[key] = [0.0332, 0.0306, 0.1211,0.039]\n",
    "                    pileup[key] = [0.011,0.019, 0.018,0.025]\n",
    "\n",
    "\n",
    "                lumi[key] = [0.018]*bins\n",
    "\n",
    "\n",
    "\n",
    "                ggH_higgsPt[key], VBFH_higgsPt[key], WH_higgsPt[key] , ZH_higgsPt[key] , ttH_higgsPt[key] , ggZH_higgsPt[key] = ([0.0]*bins*2,)*6\n",
    "                ggH_xsec[key], VBFH_xsec[key], WH_xsec[key] , ZH_xsec[key] , ttH_xsec[key] , ggZH_xsec[key] = ([0.0]*bins*2,)*6\n",
    "                ggH_pdf[key] , VBFH_pdf[key] , WH_pdf[key] , ZH_pdf[key] , ttH_pdf[key] , ggZH_pdf[key], scaling[key] = ([0.0]*bins,)*7\n",
    "                \n",
    "    #                 if category == 2 or datacard_version == 'v1': ggH_higgsPt[key] = [0.205, 0.205,0.207,0.208, 0.133,0.133,0.134,0.134] #down*4/up*4\n",
    "    #                 else:  ggH_higgsPt[key] = [0.205, 0.205,0.207, 0.133,0.133,0.134] #down*4/up*4\n",
    "\n",
    "                if category == 0:ggH_higgsPt[key] = [0.2066,0.2069, 0.2069,0.1339,0.134,0.1341] #down*3/up*3\n",
    "                elif category == 1:ggH_higgsPt[key] = [0.2074,0.2077, 0.2061,0.1342,0.1343,0.1336] #down*3/up*3\n",
    "                else:ggH_higgsPt[key] = [0.2071, 0.2066, 0.2084, 0.2076, 0.134,0.134,0.1345,0.1343] #down*4/up*4\n",
    "                ggH_xsec[key] = [0.067]*bins+[0.046]*bins #down/up\n",
    "                ggH_pdf[key] = [0.032]*bins\n",
    "\n",
    "\n",
    "\n",
    "sig_eff = [lumi, cscCluster_mc, cscCluster_data, time_spread_mc, time_spread_data, dtCluster, JES,pileup, \\\n",
    "           ggH_higgsPt, VBFH_higgsPt, WH_higgsPt, ZH_higgsPt, ttH_higgsPt, ggZH_higgsPt,  \\\n",
    "           ggH_xsec, VBFH_xsec, WH_xsec, ZH_xsec, ttH_xsec, ggZH_xsec, \\\n",
    "           ggH_pdf,VBFH_pdf, WH_pdf, ZH_pdf, ttH_pdf, ggZH_pdf,\\\n",
    "           scaling, mc_stats]\n",
    "sig_eff_name = ['lumi','cscCluster_nhits_mc', 'cscCluster_nhits_data', 'csc_timeSpread_mc', 'csc_timeSpread_data', 'dtCluster_nhits', 'JES','pileup', \\\n",
    "                'ggH_higgsPt', 'VBFH_higgsPt', 'WH_higgsPt', 'ZH_higgsPt', 'ttH_higgsPt', 'ggZH_higgsPt',\\\n",
    "                'ggH_xsec', 'VBFH_xsec', 'WH_xsec', 'ZH_xsec', 'ttH_xsec', 'ggZH_xsec', \\\n",
    "                'ggH_pdf','VBFH_pdf', 'WH_pdf', 'ZH_pdf', 'ttH_pdf', 'ggZH_pdf',\\\n",
    "                'ggZH_reweight','category'+str(category)+'mc_stats']\n",
    "\n",
    "\n",
    "# csc: jet, muon, time, time spread, clustereff_unc, readout\n",
    "# theory uncertainty for now\n",
    "assert(len(sig_eff)==len(sig_eff_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make datacard (ABCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_10_2_13/src/HiggsAnalysis/MuonSystemLimit/combine/datacards_2tag/V1p17/v2/v163//v11/unblindABC/category1/\n",
      "category: 1\n",
      "background:  [0.08333333 1.         3.        ]\n",
      "[0.08333333 1.         3.        ] [0.08333333 1.         3.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "UNBLIND = 1\n",
    "# 0 use exponential fit, 1 unblind ABC, 2 unblind ABCD\n",
    "N_RECHIT_CSC = 100\n",
    "N_RECHIT_DT = 80\n",
    "\n",
    "\n",
    "#####\n",
    "# a = c*c1*c2\n",
    "# b = c1* c\n",
    "# c = c\n",
    "# d = c2*c\n",
    "# start adding signal\n",
    "#####\n",
    "outDataCardsDir = '/storage/af/user/christiw/login-1/christiw/LLP/CMSSW_10_2_13/src/HiggsAnalysis/MuonSystemLimit/combine/datacards_2tag/'+\\\n",
    "ntupler_version+analyzer_version+'/'\n",
    "outDataCardsDir += datacard_version +'/'\n",
    "if UNBLIND == 0: outDataCardsDir += 'blind/'\n",
    "elif UNBLIND == 1: outDataCardsDir += 'unblindABC/'\n",
    "else:outDataCardsDir += 'unblind/'\n",
    "outDataCardsDir += 'category'+str(category)+'/'\n",
    "if not os.path.isdir(outDataCardsDir):os.makedirs(outDataCardsDir)\n",
    "print(outDataCardsDir)\n",
    "if category == 2 or datacard_version == 'v1': bins = 4\n",
    "else: bins = 3\n",
    "\n",
    "\n",
    "#############\n",
    "# BACKGROUND\n",
    "#############\n",
    "\n",
    "bkg_unc = [] #ABCD closure, sum of stats from two VR\n",
    "bkg_unc_name = []\n",
    "if UNBLIND == 0:\n",
    "    ##### v138 #####\n",
    "    if 'v138' in analyzer_version:\n",
    "        if category == 0:bkg_rate = np.array([0.062*0.073,0.073,1,0.062])*59 #2csc\n",
    "        elif category == 1:bkg_rate = np.array([0.02*0.01,0.02,1,0.01])*44 # dt\n",
    "        elif category == 2:bkg_rate = np.array([0.07*0.15,0.07,1,0.15])*66 #csc+dt\n",
    "    elif 'v141' in analyzer_version:\n",
    "        if datacard_version == 'v1':\n",
    "            if category == 0:bkg_rate = np.array([0.03*0.03,0.03,1 ,0.03])*18 #2csc\n",
    "            elif category == 1:bkg_rate = np.array([0.03*0.03,0.03,1, 0.03])*19 # dt\n",
    "            elif category == 2:bkg_rate = np.array([0.009*0.025,0.009,1,0.025])*35 #csc+dt\n",
    "        elif datacard_version == 'v2':\n",
    "            if category == 0:bkg_rate = np.array([0.03*0.03,0.03*2,1])*18 #2csc\n",
    "            elif category == 1:bkg_rate = np.array([0.03*0.03,0.03*2,1])*19 # dt\n",
    "            elif category == 2:bkg_rate = np.array([0.009*0.025,0.009,1,0.025])*35 #csc+dt\n",
    "    elif 'v146' in analyzer_version: #with dtcosmic muon veto\n",
    "        if datacard_version == 'v2':\n",
    "            if category == 0:bkg_rate = np.array([0.09, 2.5, 18])\n",
    "            elif category == 1:bkg_rate = np.array([0.02, 1.2, 17.9])\n",
    "            elif category == 2:bkg_rate = np.array([0.02, 0.7, 34.2, 0.9])\n",
    "elif UNBLIND == 1:\n",
    "    k = 'data'\n",
    "    if category <= 1:\n",
    "        if category == 0: N_RECHIT_CUT = N_RECHIT_CSC\n",
    "        else: N_RECHIT_CUT = N_RECHIT_DT\n",
    "#             cond_a = np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT)#bin A\n",
    "        b = np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT)#bin A\n",
    "        b = np.count_nonzero(np.logical_or(b, np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT)))\n",
    "        c = np.count_nonzero(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT))\n",
    "        bkg_rate = np.array([b**2/4.0/c, b, c])\n",
    "\n",
    "    else:\n",
    "        N_RECHIT_CUT_1 = N_RECHIT_DT\n",
    "        N_RECHIT_CUT_2 = N_RECHIT_CSC\n",
    "        cond_a = np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2)#bin A\n",
    "        b = np.count_nonzero(np.logical_and(nhits1[k]<N_RECHIT_CUT_1, nhits2[k]>=N_RECHIT_CUT_2))\n",
    "        d = np.count_nonzero(np.logical_and(nhits1[k]>=N_RECHIT_CUT_1, nhits2[k]<N_RECHIT_CUT_2))\n",
    "        c = np.count_nonzero(np.logical_and(nhits1[k]<N_RECHIT_CUT_1, nhits2[k]<N_RECHIT_CUT_2))\n",
    "        bkg_rate = np.array([b*d/c, b, c, d])\n",
    "    print('category: '+str(category))\n",
    "    print('background: ', bkg_rate)\n",
    "\n",
    "else: assert(False)\n",
    "observation= bkg_rate #blinded observation\n",
    "print(observation, bkg_rate)\n",
    "#############\n",
    "# SIGNAL\n",
    "#############\n",
    "\n",
    "\n",
    "sig_norm = []\n",
    "for k in weight.keys():\n",
    "    modelName = k + '_nCsc'+str(N_RECHIT_CSC)+'_nDt'+str(N_RECHIT_DT)\n",
    "\n",
    "    signal_rate = {}\n",
    "    mc_stat_unc = {}\n",
    "    gmn = {}\n",
    "    sig_unc = {}\n",
    "\n",
    "\n",
    "    signal_rate['ggH'] = np.zeros((bins,))\n",
    "    mc_stat_unc['ggH'] = np.zeros((bins,))\n",
    "    gmn['ggH'] = np.zeros((bins,))\n",
    "    sig_unc['ggH'] = []\n",
    "\n",
    "    w = weight[k]\n",
    "    weight_cond = np.ones(nhits2[k].shape)\n",
    "    if category == 2:\n",
    "        a = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_DT, nhits2[k]>=N_RECHIT_CSC), weight_cond)\n",
    "        signal_rate['ggH'][0]+=np.sum(w[a])\n",
    "        mc_stat_unc['ggH'][0]+=np.sum(w[a]**2)\n",
    "        gmn['ggH'][0] += len(w[a])\n",
    "        b = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_DT, nhits2[k]>=N_RECHIT_CSC), weight_cond)\n",
    "        signal_rate['ggH'][1]+=np.sum(w[b])\n",
    "        mc_stat_unc['ggH'][1]+=np.sum(w[b]**2)\n",
    "        gmn['ggH'][1] += len(w[b])\n",
    "        c = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_DT, nhits2[k]<N_RECHIT_CSC), weight_cond)\n",
    "        signal_rate['ggH'][2]+=np.sum(w[c])\n",
    "        mc_stat_unc['ggH'][2]+=np.sum(w[c]**2)\n",
    "        gmn['ggH'][2] += len(w[c])\n",
    "        d = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_DT, nhits2[k]<N_RECHIT_CSC), weight_cond)\n",
    "        signal_rate['ggH'][3]+=np.sum(w[d])\n",
    "        mc_stat_unc['ggH'][3]+=np.sum(w[d]**2)\n",
    "        gmn['ggH'][3] += len(w[d])\n",
    "    elif datacard_version == 'v1':\n",
    "        if category == 0:N_RECHIT_CUT = N_RECHIT_CSC\n",
    "        else: N_RECHIT_CUT = N_RECHIT_DT\n",
    "\n",
    "        a = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT), weight_cond)\n",
    "        signal_rate['ggH'][0]+=np.sum(w[a])\n",
    "        mc_stat_unc['ggH'][0]+=np.sum(w[a]**2)\n",
    "        gmn['ggH'][0] += len(w[a])\n",
    "        b = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT), weight_cond)\n",
    "        signal_rate['ggH'][1]+=np.sum(w[b])\n",
    "        mc_stat_unc['ggH'][1]+=np.sum(w[b]**2)\n",
    "        gmn['ggH'][1] += len(w[b])\n",
    "        c = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT), weight_cond)\n",
    "        signal_rate['ggH'][2]+=np.sum(w[c])\n",
    "        mc_stat_unc['ggH'][2]+=np.sum(w[c]**2)\n",
    "        gmn['ggH'][2] += len(w[c])\n",
    "        d = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT), weight_cond)\n",
    "        signal_rate['ggH'][3]+=np.sum(w[d])\n",
    "        mc_stat_unc['ggH'][3]+=np.sum(w[d]**2)\n",
    "        gmn['ggH'][3] += len(w[d])\n",
    "\n",
    "    else:\n",
    "        ####\n",
    "        # a is signal region, b has one cluster pass Nrechit cut, C has no cluster passing Nrechit cut\n",
    "        ####\n",
    "        if category == 0:N_RECHIT_CUT = N_RECHIT_CSC\n",
    "        else: N_RECHIT_CUT = N_RECHIT_DT\n",
    "        a = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT), weight_cond)\n",
    "        signal_rate['ggH'][0]+=np.sum(w[a])\n",
    "        mc_stat_unc['ggH'][0]+=np.sum(w[a]**2)\n",
    "        gmn['ggH'][0] += len(w[a])\n",
    "\n",
    "        b = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]>=N_RECHIT_CUT), weight_cond)\n",
    "        d = np.logical_and(np.logical_and(nhits1[k]>=N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT), weight_cond)\n",
    "        b = np.logical_or(b, d)\n",
    "        signal_rate['ggH'][1]+=np.sum(w[b])\n",
    "        mc_stat_unc['ggH'][1]+=np.sum(w[b]**2)\n",
    "        gmn['ggH'][1] += len(w[b])\n",
    "\n",
    "        c = np.logical_and(np.logical_and(nhits1[k]<N_RECHIT_CUT, nhits2[k]<N_RECHIT_CUT), weight_cond)\n",
    "        signal_rate['ggH'][2]+=np.sum(w[c])\n",
    "        mc_stat_unc['ggH'][2]+=np.sum(w[c]**2)\n",
    "        gmn['ggH'][2] += len(w[c])\n",
    "\n",
    "    for j in range(bins):\n",
    "        if signal_rate['ggH'][j]<0.0:\n",
    "            signal_rate['ggH'][j]=0.0\n",
    "            mc_stat_unc['ggH'][j]=0.0\n",
    "            gmn['ggH'][j] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    mc_stat_unc['ggH'] = np.nan_to_num(signal_rate['ggH']/np.sqrt(mc_stat_unc['ggH']))**2 #gmn\n",
    "    mc_stat_unc['ggH'][mc_stat_unc['ggH'] == np.inf] = 0.0\n",
    "\n",
    "    mc_stat_unc['ggH'] = list(mc_stat_unc['ggH'])\n",
    "    mc_stat_unc['ggH'] = list(gmn['ggH'])\n",
    "    if p == 'others': k = k.replace(\"ggH\", \"others\")\n",
    "    for j, ele in enumerate(sig_eff):# go through each uncertainty\n",
    "        if j == len(sig_eff)-1: # if mc_stats\n",
    "            sig_unc['ggH'].append(mc_stat_unc['ggH'])\n",
    "        else:\n",
    "            sig_unc['ggH'].append(ele[k])\n",
    "\n",
    "\n",
    "    norm = np.sum(signal_rate['ggH'])/4\n",
    "    #only used for calculation, the signal yield is not changed in datacard\n",
    "\n",
    "\n",
    "    if category == 2 or datacard_version == 'v1': make_datacard_2tag(outDataCardsDir, modelName, signal_rate, norm, bkg_rate, observation, \\\n",
    "                      bkg_unc, bkg_unc_name, sig_unc, sig_eff_name, 'a', 'category'+str(category)+'_')\n",
    "    else: make_datacard_2tag_3bins(outDataCardsDir, modelName, signal_rate, norm, bkg_rate, observation, \\\n",
    "                          bkg_unc, bkg_unc_name, sig_unc, sig_eff_name, 'a', 'category'+str(category)+'_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unblind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dphiMet_cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5c4a17b991ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_ev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdphiMet_cluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mDPHI_CUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nrechits, A, B, C, pred, D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dphiMet_cluster' is not defined"
     ]
    }
   ],
   "source": [
    "k = 'data_intime_sr'\n",
    "\n",
    "n_ev = 5000\n",
    "var = np.abs(dphiMet_cluster[k])\n",
    "DPHI_CUT = 0.75\n",
    "print(\"Nrechits, A, B, C, pred, D\")\n",
    "for N_RECHIT_CUT in np.arange(60,240,10):\n",
    "    a = np.count_nonzero(np.logical_and(nCsc_JetMuonVetoCluster0p4_Me1112Veto[k]>=N_RECHIT_CUT, var>=DPHI_CUT))\n",
    "    b = np.count_nonzero(np.logical_and(nCsc_JetMuonVetoCluster0p4_Me1112Veto[k]<N_RECHIT_CUT, var>=DPHI_CUT))\n",
    "    c = np.count_nonzero(np.logical_and(nCsc_JetMuonVetoCluster0p4_Me1112Veto[k]<N_RECHIT_CUT, var<DPHI_CUT))\n",
    "    d = np.count_nonzero(np.logical_and(nCsc_JetMuonVetoCluster0p4_Me1112Veto[k]>=N_RECHIT_CUT, var<DPHI_CUT))\n",
    "    pred = c/b*a\n",
    "    unc_pred = (1./c + 1./b + 1./a)**0.5*(c/b*a)\n",
    "\n",
    "    print(N_RECHIT_CUT, '\\t',a,'\\t',b,'\\t',c,'\\t',d,'\\t', round(c/b*a, 2), '\\t',\\\n",
    "          round( (1./c + 1./b + 1./a)**0.5*(c/b*a), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
